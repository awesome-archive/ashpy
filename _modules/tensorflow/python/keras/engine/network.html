

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>tensorflow.python.keras.engine.network &mdash; AshPy 1.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/graphviz.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../../../index.html" class="icon icon-home"> AshPy
          

          
          </a>

          
            
            
              <div class="version">
                1.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../home.html">Welcome To AshPy!</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../home.html#ashpy">AshPy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../home.html#set-up">Set up</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../home.html#ashpy-usage">AshPy usage</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../home.html#classifier">Classifier</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../home.html#gan-generative-adversarial-network">GAN - Generative Adversarial Network</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../home.html#dataset-output-format">Dataset Output Format</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../home.html#executor-context-metric-and-strategies">Executor, Context, Metric and Strategies</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../write_the_docs.html">Write The Docs!</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../write_the_docs.html#the-whys">The Whys</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../write_the_docs.html#why-sphinx">Why Sphinx?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../write_the_docs.html#why-rest">Why reST?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../write_the_docs.html#why-google-style-for-docstrings">Why Google Style for Docstrings?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../write_the_docs.html#documentation-architecture">Documentation Architecture</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../write_the_docs.html#tutorials-guides-complex-examples">Tutorials, Guides, Complex Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../write_the_docs.html#api-reference">API Reference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../write_the_docs.html#automate-all-the-docs">Automate all the docs!</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../write_the_docs.html#autosummary-submodules-with-imports-a-painful-story">Autosummary &amp; submodules with imports: A painful story</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../write_the_docs.html#inheritance-diagrams">Inheritance Diagrams</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../write_the_docs.html#additional-materials">Additional Materials</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../getting_started.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../getting_started.html#creating-a-new-dataset">Creating a new Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../getting_started.html#creating-a-new-model">Creating a new Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../getting_started.html#creating-a-new-trainer">Creating a new Trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../getting_started.html#complete-examples">Complete Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../getting_started.html#classifier">Classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../getting_started.html#gans">GANs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../getting_started.html#bigan">BiGAN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../getting_started.html#mnist">MNIST</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../api.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../_autosummary/ashpy.models.html">ashpy.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/models/ashpy.models.convolutional.html">convolutional</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/convolutional/ashpy.models.convolutional.interfaces.Conv2DInterface.html">Conv2DInterface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/convolutional/ashpy.models.convolutional.decoders.BaseDecoder.html">BaseDecoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/convolutional/ashpy.models.convolutional.decoders.FCNNBaseDecoder.html">FCNNBaseDecoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/convolutional/ashpy.models.convolutional.encoders.BaseEncoder.html">BaseEncoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/convolutional/ashpy.models.convolutional.encoders.FCNNBaseEncoder.html">FCNNBaseEncoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/fc/ashpy.models.convolutional.autoencoders.BaseAutoencoder.html">BaseAutoencoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/fc/ashpy.models.convolutional.autoencoders.FCNNBaseAutoencoder.html">FCNNBaseAutoencoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/convolutional/ashpy.models.convolutional.interfaces.html">interfaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/convolutional/ashpy.models.convolutional.decoders.html">decoders</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/convolutional/ashpy.models.convolutional.encoders.html">encoders</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/convolutional/ashpy.models.convolutional.autoencoders.html">autoencoders</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/models/ashpy.models.fc.html">fc</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/fc/ashpy.models.fc.interfaces.FCInterface.html">FCInterface</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/fc/ashpy.models.fc.decoders.BaseDecoder.html">BaseDecoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/fc/ashpy.models.fc.encoders.BaseEncoder.html">BaseEncoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/fc/ashpy.models.fc.autoencoders.BaseAutoencoder.html">BaseAutoencoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/fc/ashpy.models.fc.interfaces.html">interfaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/fc/ashpy.models.fc.decoders.html">decoders</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/fc/ashpy.models.fc.encoders.html">encoders</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/fc/ashpy.models.fc.autoencoders.html">autoencoders</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/models/ashpy.models.gans.html">gans</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/models/ashpy.models.gans.Generator.html">Generator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/models/ashpy.models.gans.Discriminator.html">Discriminator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/models/models/ashpy.models.gans.Encoder.html">Encoder</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../_autosummary/ashpy.contexts.html">ashpy.contexts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/contexts/ashpy.contexts.base_context.BaseContext.html">BaseContext</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/contexts/ashpy.contexts.classifier.ClassifierContext.html">ClassifierContext</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/contexts/ashpy.contexts.gan.GANContext.html">GANContext</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/contexts/ashpy.contexts.gan.GANEncoderContext.html">GANEncoderContext</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/contexts/ashpy.contexts.base_context.html">base_context</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/contexts/base_context/ashpy.contexts.base_context.BaseContext.html">BaseContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/contexts/ashpy.contexts.classifier.html">classifier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/contexts/classifier/ashpy.contexts.classifier.ClassifierContext.html">ClassifierContext</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/contexts/ashpy.contexts.gan.html">gan</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/contexts/gan/ashpy.contexts.gan.GANContext.html">GANContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/contexts/gan/ashpy.contexts.gan.GANEncoderContext.html">GANEncoderContext</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../_autosummary/ashpy.trainers.html">ashpy.trainers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/trainers/ashpy.trainers.BaseTrainer.html">BaseTrainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/trainers/ashpy.trainers.AdversarialTrainer.html">AdversarialTrainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/trainers/ashpy.trainers.EncoderTrainer.html">EncoderTrainer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/trainers/ashpy.trainers.base_trainer.html">base_trainer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/trainers/base_trainer/ashpy.trainers.base_trainer.BaseTrainer.html">BaseTrainer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/trainers/ashpy.trainers.classifier.html">classifier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/trainers/classifier/ashpy.trainers.classifier.ClassifierTrainer.html">ClassifierTrainer</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/trainers/ashpy.trainers.gan.html">gan</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/trainers/gan/ashpy.trainers.gan.AdversarialTrainer.html">AdversarialTrainer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/trainers/gan/ashpy.trainers.gan.EncoderTrainer.html">EncoderTrainer</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../_autosummary/ashpy.layers.html">ashpy.layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/layers/ashpy.layers.instance_normalization.InstanceNormalization.html">InstanceNormalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/layers/ashpy.layers.attention.Attention.html">Attention</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../_autosummary/ashpy.losses.html">ashpy.losses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/executor/ashpy.losses.executor.Executor.html">Executor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/executor/ashpy.losses.executor.SumExecutor.html">SumExecutor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/classifier/ashpy.losses.classifier.ClassifierLoss.html">ClassifierLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.AdversarialLossType.html">AdversarialLossType</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.AdversarialLossG.html">AdversarialLossG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.AdversarialLossD.html">AdversarialLossD</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.GeneratorBCE.html">GeneratorBCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.GeneratorLSGAN.html">GeneratorLSGAN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.GeneratorL1.html">GeneratorL1</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.FeatureMatchingLoss.html">FeatureMatchingLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.CategoricalCrossEntropy.html">CategoricalCrossEntropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.Pix2PixLoss.html">Pix2PixLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.Pix2PixLossSemantic.html">Pix2PixLossSemantic</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.EncoderBCE.html">EncoderBCE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.DiscriminatorMinMax.html">DiscriminatorMinMax</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.DiscriminatorLSGAN.html">DiscriminatorLSGAN</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.get_adversarial_loss_discriminator.html">ashpy.losses.gan.get_adversarial_loss_discriminator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.losses.gan.get_adversarial_loss_generator.html">ashpy.losses.gan.get_adversarial_loss_generator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/losses/ashpy.losses.classifier.html">classifier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/classifier/ashpy.losses.classifier.ClassifierLoss.html">ClassifierLoss</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/losses/ashpy.losses.executor.html">executor</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/executor/ashpy.losses.executor.Executor.html">Executor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/executor/ashpy.losses.executor.SumExecutor.html">SumExecutor</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/losses/ashpy.losses.gan.html">gan</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.get_adversarial_loss_discriminator.html">ashpy.losses.gan.get_adversarial_loss_discriminator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.get_adversarial_loss_generator.html">ashpy.losses.gan.get_adversarial_loss_generator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.AdversarialLossD.html">AdversarialLossD</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.AdversarialLossG.html">AdversarialLossG</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.AdversarialLossType.html">AdversarialLossType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.CategoricalCrossEntropy.html">CategoricalCrossEntropy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.DiscriminatorLSGAN.html">DiscriminatorLSGAN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.DiscriminatorMinMax.html">DiscriminatorMinMax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.EncoderBCE.html">EncoderBCE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.FeatureMatchingLoss.html">FeatureMatchingLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.GANExecutor.html">GANExecutor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.GeneratorBCE.html">GeneratorBCE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.GeneratorL1.html">GeneratorL1</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.GeneratorLSGAN.html">GeneratorLSGAN</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.Pix2PixLoss.html">Pix2PixLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/losses/gan/ashpy.losses.gan.Pix2PixLossSemantic.html">Pix2PixLossSemantic</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../_autosummary/ashpy.metrics.html">ashpy.metrics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/metric/ashpy.metrics.metric.Metric.html">Metric</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/classifier/ashpy.metrics.classifier.ClassifierLoss.html">ClassifierLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/classifier/ashpy.metrics.classifier.ClassifierMetric.html">ClassifierMetric</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.metrics.gan.DiscriminatorLoss.html">DiscriminatorLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.metrics.gan.GeneratorLoss.html">GeneratorLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.metrics.gan.EncoderLoss.html">EncoderLoss</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.metrics.gan.InceptionScore.html">InceptionScore</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/gan/ashpy.metrics.gan.EncodingAccuracy.html">EncodingAccuracy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/metrics/ashpy.metrics.classifier.html">classifier</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/metrics/classifier/ashpy.metrics.classifier.ClassifierLoss.html">ClassifierLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/metrics/classifier/ashpy.metrics.classifier.ClassifierMetric.html">ClassifierMetric</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/metrics/ashpy.metrics.gan.html">gan</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/metrics/gan/ashpy.metrics.gan.DiscriminatorLoss.html">DiscriminatorLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/metrics/gan/ashpy.metrics.gan.EncoderLoss.html">EncoderLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/metrics/gan/ashpy.metrics.gan.EncodingAccuracy.html">EncodingAccuracy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/metrics/gan/ashpy.metrics.gan.GeneratorLoss.html">GeneratorLoss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/metrics/gan/ashpy.metrics.gan.InceptionScore.html">InceptionScore</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../_autosummary/metrics/ashpy.metrics.metric.html">metric</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../../_autosummary/metrics/metric/ashpy.metrics.metric.Metric.html">Metric</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../dependencies_graph.html">Dependencies Graph</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../dependencies_graph.html#ashpy-models">ashpy.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../dependencies_graph.html#convolutional">Convolutional</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../dependencies_graph.html#gans">GANs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../dependencies_graph.html#ashpy-trainers">ashpy.trainers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../dependencies_graph.html#adversarial">Adversarial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../dependencies_graph.html#classifier">Classifier</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../dependencies_graph.html#ashpy-layers">ashpy.layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../dependencies_graph.html#layers">Layers</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">AshPy</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../../../index.html">Module code</a> &raquo;</li>
        
      <li>tensorflow.python.keras.engine.network</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for tensorflow.python.keras.engine.network</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="c1"># pylint: disable=protected-access</span>
<span class="sd">&quot;&quot;&quot;A `Network` is way to compose layers: the topological form of a `Model`.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="k">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">six.moves</span> <span class="k">import</span> <span class="nb">zip</span>  <span class="c1"># pylint: disable=redefined-builtin</span>

<span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="k">import</span> <span class="n">pywrap_tensorflow</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.eager</span> <span class="k">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">errors</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">errors_impl</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">func_graph</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="k">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="k">import</span> <span class="n">backend</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="k">import</span> <span class="n">saving</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="k">import</span> <span class="n">base_layer</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="k">import</span> <span class="n">base_layer_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="k">import</span> <span class="n">training_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.mixed_precision.experimental</span> <span class="k">import</span> <span class="n">policy</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils</span> <span class="k">import</span> <span class="n">generic_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils</span> <span class="k">import</span> <span class="n">layer_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils</span> <span class="k">import</span> <span class="n">tf_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.utils.io_utils</span> <span class="k">import</span> <span class="n">ask_to_proceed_with_overwrite</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="k">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training</span> <span class="k">import</span> <span class="n">checkpoint_management</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">base</span> <span class="k">as</span> <span class="n">trackable</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">data_structures</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">layer_utils</span> <span class="k">as</span> <span class="n">trackable_layer_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">tracking</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.training.tracking</span> <span class="k">import</span> <span class="n">util</span> <span class="k">as</span> <span class="n">trackable_utils</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">nest</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">serialization</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="k">import</span> <span class="n">tf_inspect</span>


<span class="c1"># pylint: disable=g-import-not-at-top</span>
<span class="k">try</span><span class="p">:</span>
  <span class="kn">import</span> <span class="nn">h5py</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
  <span class="n">h5py</span> <span class="o">=</span> <span class="kc">None</span>

<span class="k">try</span><span class="p">:</span>
  <span class="kn">import</span> <span class="nn">yaml</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
  <span class="n">yaml</span> <span class="o">=</span> <span class="kc">None</span>
<span class="c1"># pylint: enable=g-import-not-at-top</span>


<span class="k">class</span> <span class="nc">Network</span><span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;A `Network` is a composition of layers.</span>

<span class="sd">  `Network` is the topological form of a &quot;model&quot;. A `Model`</span>
<span class="sd">  is simply a `Network` with added training routines.</span>

<span class="sd">  Two types of `Networks` exist: Graph Networks and Subclass Networks. Graph</span>
<span class="sd">  networks are used in the Keras Functional and Sequential APIs. Subclassed</span>
<span class="sd">  networks are used when a user subclasses the `Model` class. In general,</span>
<span class="sd">  more Keras features are supported with Graph Networks than with Subclassed</span>
<span class="sd">  Networks, specifically:</span>

<span class="sd">  - Model cloning (`keras.models.clone`)</span>
<span class="sd">  - Serialization (`model.get_config()/from_config`, `model.to_json()/to_yaml()`</span>
<span class="sd">  - Whole-model saving (`model.save()`)</span>

<span class="sd">  A Graph Network can be instantiated by passing two arguments to `__init__`.</span>
<span class="sd">  The first argument is the `keras.Input` Tensors that represent the inputs</span>
<span class="sd">  to the Network. The second argument specifies the output Tensors that</span>
<span class="sd">  represent the outputs of this Network. Both arguments can be a nested</span>
<span class="sd">  structure of Tensors.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```</span>
<span class="sd">  inputs = {&#39;x1&#39;: keras.Input(shape=(10,)), &#39;x2&#39;: keras.Input(shape=(1,))}</span>
<span class="sd">  t = keras.layers.Dense(1, activation=&#39;relu&#39;)(inputs[&#39;x1&#39;])</span>
<span class="sd">  outputs = keras.layers.Add()([t, inputs[&#39;x2&#39;])</span>
<span class="sd">  network = Network(inputs, outputs)</span>
<span class="sd">  ```</span>

<span class="sd">  A Graph Network constructed using the Functional API can also include raw</span>
<span class="sd">  TensorFlow functions, with the exception of functions that create Variables</span>
<span class="sd">  or assign ops.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```</span>
<span class="sd">  inputs = keras.Input(shape=(10,))</span>
<span class="sd">  x = keras.layers.Dense(1)(inputs)</span>
<span class="sd">  outputs = tf.nn.relu(x)</span>
<span class="sd">  network = Network(inputs, outputs)</span>
<span class="sd">  ```</span>

<span class="sd">  Subclassed Networks can be instantiated via `name` and (optional) `dynamic`</span>
<span class="sd">  keyword arguments. Subclassed Networks keep track of their Layers, and their</span>
<span class="sd">  `call` method can be overridden. Subclassed Networks are typically created</span>
<span class="sd">  indirectly, by subclassing the `Model` class.</span>

<span class="sd">  Example:</span>

<span class="sd">  ```</span>
<span class="sd">  class MyModel(keras.Model):</span>
<span class="sd">    def __init__(self):</span>
<span class="sd">      super(MyModel, self).__init__(name=&#39;my_model&#39;, dynamic=False)</span>

<span class="sd">      self.layer1 = keras.layers.Dense(10, activation=&#39;relu&#39;)</span>

<span class="sd">    def call(self, inputs):</span>
<span class="sd">      return self.layer1(inputs)</span>
<span class="sd">  ```</span>

<span class="sd">  Allowed args in `super().__init__`:</span>
<span class="sd">    name: String name of the model.</span>
<span class="sd">    dynamic: (Subclassed models only) Set this to `True` if your model should</span>
<span class="sd">      only be run eagerly, and should not be used to generate a static</span>
<span class="sd">      computation graph. This attribute is automatically set for Functional API</span>
<span class="sd">      models.</span>
<span class="sd">    trainable: Boolean, whether the model&#39;s variables should be trainable.</span>
<span class="sd">    dtype: (Subclassed models only) Default dtype of the model&#39;s weights (</span>
<span class="sd">      default of `None` means use the type of the first input). This attribute</span>
<span class="sd">      has no effect on Functional API models, which do not have weights of their</span>
<span class="sd">      own.</span>
<span class="sd">  &quot;&quot;&quot;</span>

  <span class="c1"># See tf.Module for the usage of this property.</span>
  <span class="c1"># The key of _layer_call_argspecs is a layer. tf.Module._flatten will fail to</span>
  <span class="c1"># flatten the key since it is trying to convert Trackable/Layer to a string.</span>
  <span class="n">_TF_MODULE_IGNORED_PROPERTIES</span> <span class="o">=</span> <span class="nb">frozenset</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
      <span class="p">(</span><span class="s1">&#39;_layer_call_argspecs&#39;</span><span class="p">,),</span>
      <span class="n">base_layer</span><span class="o">.</span><span class="n">Layer</span><span class="o">.</span><span class="n">_TF_MODULE_IGNORED_PROPERTIES</span>
  <span class="p">))</span>

  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>  <span class="c1"># pylint: disable=super-init-not-called</span>
    <span class="c1"># Signature detection</span>
    <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span>
        <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="s1">&#39;outputs&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">or</span>
        <span class="s1">&#39;inputs&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="ow">and</span> <span class="s1">&#39;outputs&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">):</span>
      <span class="c1"># Graph network</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_graph_network</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="c1"># Subclassed network</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_init_subclassed_network</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">tf_utils</span><span class="o">.</span><span class="n">assert_no_legacy_layers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>

  <span class="c1"># Several Network methods have &quot;no_automatic_dependency_tracking&quot;</span>
  <span class="c1"># annotations. Since Network does automatic dependency tracking on attribute</span>
  <span class="c1"># assignment, including for common data structures such as lists, by default</span>
  <span class="c1"># we&#39;d have quite a few empty dependencies which users don&#39;t care about (or</span>
  <span class="c1"># would need some way to ignore dependencies automatically, which is confusing</span>
  <span class="c1"># when applied to user code). Some attributes, such as _layers, would cause</span>
  <span class="c1"># structural issues (_layers being the place where Layers assigned to tracked</span>
  <span class="c1"># attributes are stored).</span>
  <span class="c1">#</span>
  <span class="c1"># Aside from these aesthetic and structural issues, useless dependencies on</span>
  <span class="c1"># empty lists shouldn&#39;t cause issues; adding or removing them will not break</span>
  <span class="c1"># checkpoints, but may cause &quot;all Python objects matched&quot; assertions to fail</span>
  <span class="c1"># (in which case less strict assertions may be substituted if necessary).</span>
  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">_base_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># The following are implemented as property functions:</span>
    <span class="c1"># self.trainable_weights</span>
    <span class="c1"># self.non_trainable_weights</span>
    <span class="c1"># self.input_spec</span>
    <span class="c1"># self.losses</span>
    <span class="c1"># self.updates</span>

    <span class="n">generic_utils</span><span class="o">.</span><span class="n">validate_kwargs</span><span class="p">(</span><span class="n">kwargs</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;trainable&#39;</span><span class="p">,</span> <span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="s1">&#39;dynamic&#39;</span><span class="p">})</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_init_set_name</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">zero_based</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_activity_regularizer</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># This acts just like the `trainable` attribute of any layer instance.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_trainable</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;trainable&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
    <span class="c1"># This attribute has no effect if the model is created using the Functional</span>
    <span class="c1"># API. Instead, `model.dynamic` is determined based on the internal layers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;dynamic&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_compiled</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># This is True for Sequential networks and Functional networks.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_compute_output_and_mask_jointly</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">supports_masking</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;optimizer&#39;</span><span class="p">):</span>
      <span class="c1"># Don&#39;t reset optimizer if already set.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="c1"># Private attributes to implement compatibility with Layer.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_trainable_weights&#39;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_non_trainable_weights&#39;</span><span class="p">,</span> <span class="p">[])</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_updates</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># Used in symbolic mode only.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_eager_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_callable_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># A list of metric instances corresponding to the symbolic metric tensors</span>
    <span class="c1"># added using the `add_metric` API.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># A dictionary that maps metric names to metric result tensors.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_metrics_tensors</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_scope</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Never used.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Never used.</span>
    <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_default_graph</span><span class="p">()</span>  <span class="c1"># Used in symbolic mode only.</span>
      <span class="c1"># A Network does not create weights of its own, thus has no dtype.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dtype</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

    <span class="c1"># All layers in order of horizontal graph traversal.</span>
    <span class="c1"># Entries are unique. Includes input and output layers.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_maybe_create_attribute</span><span class="p">(</span><span class="s1">&#39;_layers&#39;</span><span class="p">,</span> <span class="p">[])</span>

    <span class="c1"># Used in symbolic mode only, only in conjunction with graph-networks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_outbound_nodes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_inbound_nodes</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_trackable_saver</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">trackable_utils</span><span class="o">.</span><span class="n">saver_with_op_caching</span><span class="p">(</span><span class="bp">self</span><span class="p">))</span>

    <span class="c1"># Networks do not need to do any casting of inputs or variables, because</span>
    <span class="c1"># each of its layers will handle casting through the layer&#39;s own</span>
    <span class="c1"># implementation. Therefore networks use the &#39;infer&#39; policy, which does no</span>
    <span class="c1"># casting.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_mixed_precision_policy</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">Policy</span><span class="p">(</span><span class="s1">&#39;infer&#39;</span><span class="p">)</span>

  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">_init_graph_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">generic_utils</span><span class="o">.</span><span class="n">validate_kwargs</span><span class="p">(</span>
        <span class="n">kwargs</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;trainable&#39;</span><span class="p">},</span>
        <span class="s1">&#39;Functional models may only specify `name` and `trainable` keyword &#39;</span>
        <span class="s1">&#39;arguments during initialization. Got an unexpected argument:&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_call_convention</span> <span class="o">=</span> <span class="p">(</span><span class="n">base_layer_utils</span>
                             <span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">EXPLICIT_INPUTS_ARGUMENT</span><span class="p">)</span>
    <span class="c1"># Normalize and set self.inputs, self.outputs.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_nested_outputs</span> <span class="o">=</span> <span class="n">outputs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_nested_inputs</span> <span class="o">=</span> <span class="n">inputs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="s1">&#39;_keras_history&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">):</span>
      <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">create_keras_history</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nested_outputs</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_base_init</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_graph_inputs_and_outputs</span><span class="p">()</span>

    <span class="c1"># A Network does not create weights of its own, thus it is already</span>
    <span class="c1"># built.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_compute_output_and_mask_jointly</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="c1"># `_expects_training_arg` is True since the `training` argument is always</span>
    <span class="c1"># present in the signature of the `call` method of a graph network.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_expects_training_arg</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_expects_mask_arg</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_input_layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_layers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_input_coordinates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_coordinates</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># This is for performance optimization when calling the Network on new</span>
    <span class="c1"># inputs. Every time the Network is called on a set on input tensors,</span>
    <span class="c1"># we compute the output tensors, output masks and output shapes in one pass,</span>
    <span class="c1"># then cache them here. When any of these outputs is queried later, we</span>
    <span class="c1"># retrieve it from there instead of recomputing it.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_mask_cache</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_tensor_cache</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape_cache</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Build self._output_layers:</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
      <span class="n">layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_keras_history</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_coordinates</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span><span class="p">))</span>

    <span class="c1"># Build self._input_layers:</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
      <span class="n">layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_keras_history</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="c1"># It&#39;s supposed to be an input layer, so only one node</span>
      <span class="c1"># and one tensor output.</span>
      <span class="k">assert</span> <span class="n">node_index</span> <span class="o">==</span> <span class="mi">0</span>
      <span class="k">assert</span> <span class="n">tensor_index</span> <span class="o">==</span> <span class="mi">0</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_input_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_input_coordinates</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span><span class="p">))</span>

    <span class="c1"># Keep track of the network&#39;s nodes and layers.</span>
    <span class="n">nodes</span><span class="p">,</span> <span class="n">nodes_by_depth</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">layers_by_depth</span> <span class="o">=</span> <span class="n">_map_graph_network</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_network_nodes</span> <span class="o">=</span> <span class="n">nodes</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_depth</span> <span class="o">=</span> <span class="n">nodes_by_depth</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span> <span class="o">=</span> <span class="n">layers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layers_by_depth</span> <span class="o">=</span> <span class="n">layers_by_depth</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_layer_call_argspecs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_layer_call_argspecs</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">_track_layers</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>

    <span class="c1"># Create the node linking internal inputs to internal outputs.</span>
    <span class="n">base_layer</span><span class="o">.</span><span class="n">Node</span><span class="p">(</span>
        <span class="n">outbound_layer</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
        <span class="n">inbound_layers</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">node_indices</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">tensor_indices</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">input_tensors</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_nested_inputs</span><span class="p">,</span>
        <span class="n">output_tensors</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_nested_outputs</span><span class="p">)</span>

    <span class="c1"># Build self.input_names and self.output_names.</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_set_output_names</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_names</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_feed_input_names</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_feed_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_feed_input_shapes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_layers</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">input_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">is_placeholder</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_feed_input_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
        <span class="c1"># Use batch_input_shape here because non-eager composite tensors may not</span>
        <span class="c1"># have a shape attribute that&#39;s meaningful (sparse, for instance, has</span>
        <span class="c1"># a tensor that&#39;s non-constant and needs to be fed). This means that</span>
        <span class="c1"># input layers that create placeholders will need to have the</span>
        <span class="c1"># batch_input_shape attr to allow for input shape validation.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_feed_input_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_batch_input_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_feed_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">input</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_set_output_names</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Assigns unique names to the Network&#39;s outputs.</span>

<span class="sd">    Output layers with multiple output tensors would otherwise lead to duplicate</span>
<span class="sd">    names in self.output_names.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">uniquified</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">output_names</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">prefix_count</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_layers</span><span class="p">:</span>
      <span class="n">proposal</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span>
      <span class="k">while</span> <span class="n">proposal</span> <span class="ow">in</span> <span class="n">output_names</span><span class="p">:</span>
        <span class="n">existing_count</span> <span class="o">=</span> <span class="n">prefix_count</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">_</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">existing_count</span><span class="p">)</span>
        <span class="n">prefix_count</span><span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">existing_count</span> <span class="o">+</span> <span class="mi">1</span>
      <span class="n">output_names</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>
      <span class="n">uniquified</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output_names</span> <span class="o">=</span> <span class="n">uniquified</span>

  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">_init_subclassed_network</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_base_init</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_expects_training_arg</span> <span class="o">=</span> <span class="s1">&#39;training&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_expects_mask_arg</span> <span class="o">=</span> <span class="s1">&#39;mask&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_fn_args</span>
    <span class="n">call_argspec</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_call_convention</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_determine_call_convention</span><span class="p">(</span><span class="n">call_argspec</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">False</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">dynamic</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">dynamic</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_dynamic</span> <span class="ow">or</span> <span class="nb">any</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">dynamic</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_determine_call_convention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">call_argspec</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Decides how `self.call()` is invoked. See `CallConvention`.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">call_argspec</span><span class="o">.</span><span class="n">varargs</span><span class="p">:</span>
      <span class="n">may_take_single_argument</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Note: tf_inspect doesn&#39;t raise a TypeError when regular inspect would,</span>
        <span class="c1"># so we need to keep in mind that &quot;getcallargs&quot; may have returned</span>
        <span class="c1"># something even though we under-specified positional arguments.</span>
        <span class="n">all_args</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getcallargs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="n">self_args</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">obj</span> <span class="ow">in</span> <span class="n">all_args</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
          <span class="k">if</span> <span class="n">obj</span> <span class="ow">is</span> <span class="bp">self</span><span class="p">:</span>
            <span class="n">self_args</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">arg_name</span><span class="p">)</span>
        <span class="n">may_take_single_argument</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
        <span class="n">may_take_single_argument</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">may_take_single_argument</span><span class="p">:</span>
      <span class="c1"># A single positional argument (plus &quot;self&quot;) is considered equivalent to</span>
      <span class="c1"># an &quot;inputs&quot; argument.</span>
      <span class="n">all_positional_args</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">call_argspec</span><span class="o">.</span><span class="n">args</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">call_argspec</span><span class="o">.</span><span class="n">defaults</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">all_positional_args</span> <span class="o">-=</span> <span class="nb">len</span><span class="p">(</span><span class="n">call_argspec</span><span class="o">.</span><span class="n">defaults</span><span class="p">)</span>
      <span class="n">non_self_positional_args</span> <span class="o">=</span> <span class="n">all_positional_args</span>
      <span class="k">for</span> <span class="n">positional_arg_name</span> <span class="ow">in</span> <span class="n">call_argspec</span><span class="o">.</span><span class="n">args</span><span class="p">[:</span><span class="n">all_positional_args</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">positional_arg_name</span> <span class="ow">in</span> <span class="n">self_args</span><span class="p">:</span>
          <span class="n">non_self_positional_args</span> <span class="o">-=</span> <span class="mi">1</span>
      <span class="k">if</span> <span class="n">non_self_positional_args</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="s1">&#39;inputs&#39;</span> <span class="ow">in</span> <span class="n">call_argspec</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="n">all_positional_args</span><span class="p">:]:</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
              <span class="s2">&quot;Model.call() takes a single positional argument (to which &quot;</span>
              <span class="s2">&quot;inputs are passed by convention) and a separate &#39;inputs&#39; &quot;</span>
              <span class="s2">&quot;argument. Unable to determine which arguments are inputs.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">SINGLE_POSITIONAL_ARGUMENT</span>
    <span class="k">if</span> <span class="s1">&#39;inputs&#39;</span> <span class="ow">in</span> <span class="n">call_argspec</span><span class="o">.</span><span class="n">args</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">EXPLICIT_INPUTS_ARGUMENT</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">CallConvention</span><span class="o">.</span><span class="n">POSITIONAL_ARGUMENTS_ARE_INPUTS</span>

  <span class="k">def</span> <span class="nf">_track_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Add Trackable dependencies on a list of Layers.&quot;&quot;&quot;</span>
    <span class="n">weight_layer_index</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layers</span><span class="p">):</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">weights</span><span class="p">:</span>
          <span class="c1"># Keep a separate index for layers which have weights. This allows</span>
          <span class="c1"># users to insert Layers without weights anywhere in the network</span>
          <span class="c1"># without breaking checkpoints.</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_track_trackable</span><span class="p">(</span>
              <span class="n">layer</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;layer_with_weights-</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">weight_layer_index</span><span class="p">,</span>
              <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="n">weight_layer_index</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="c1"># The layer might have weights, but may not be built yet. We just treat</span>
        <span class="c1"># it as layer without weight.</span>
        <span class="k">pass</span>

      <span class="c1"># Even if it doesn&#39;t have weights, we should still track everything in</span>
      <span class="c1"># case it has/will have Trackable dependencies.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_track_trackable</span><span class="p">(</span>
          <span class="n">layer</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;layer-</span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">layer_index</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">__setattr__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;_self_setattr_tracking&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
      <span class="nb">super</span><span class="p">(</span><span class="n">Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
      <span class="k">return</span>

    <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
        <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">(</span><span class="n">base_layer</span><span class="o">.</span><span class="n">Layer</span><span class="p">,</span>
                       <span class="n">data_structures</span><span class="o">.</span><span class="n">TrackableDataStructure</span><span class="p">))</span> <span class="ow">or</span>
        <span class="n">trackable_layer_utils</span><span class="o">.</span><span class="n">has_weights</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">value</span><span class="p">)):</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span>
      <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;It looks like you are subclassing `Model` and you &#39;</span>
                           <span class="s1">&#39;forgot to call `super(YourClass, self).__init__()`.&#39;</span>
                           <span class="s1">&#39; Always start with this line.&#39;</span><span class="p">)</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="c1"># Keep track of metric instance created in subclassed model/layer.</span>
    <span class="c1"># We do this so that we can maintain the correct order of metrics by adding</span>
    <span class="c1"># the instance to the `metrics` list as soon as it is created.</span>
    <span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="k">import</span> <span class="n">metrics</span> <span class="k">as</span> <span class="n">metrics_module</span>  <span class="c1"># pylint: disable=g-import-not-at-top</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">metrics_module</span><span class="o">.</span><span class="n">Metric</span><span class="p">):</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">stateful</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">any</span><span class="p">((</span><span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;stateful&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">layer</span><span class="o">.</span><span class="n">stateful</span><span class="p">)</span>
               <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">reset_states</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;reset_states&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;stateful&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="n">layer</span><span class="o">.</span><span class="n">reset_states</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">state_updates</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the `updates` from all layers that are stateful.</span>

<span class="sd">    This is useful for separating training updates and</span>
<span class="sd">    state updates, e.g. when we need to update a layer&#39;s internal state</span>
<span class="sd">    during prediction.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of update ops.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">state_updates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;stateful&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="s1">&#39;updates&#39;</span><span class="p">):</span>
          <span class="n">state_updates</span> <span class="o">+=</span> <span class="n">layer</span><span class="o">.</span><span class="n">updates</span>
    <span class="k">return</span> <span class="n">state_updates</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the list of all layer variables/weights.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A list of variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_assert_weights_created</span><span class="p">()</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">:</span>
      <span class="n">weights</span> <span class="o">+=</span> <span class="n">layer</span><span class="o">.</span><span class="n">weights</span>
    <span class="n">weights</span> <span class="o">+=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">weights</span>

  <span class="nd">@property</span>
  <span class="nd">@tracking</span><span class="o">.</span><span class="n">cached_per_instance</span>
  <span class="k">def</span> <span class="nf">_should_compute_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span> <span class="ow">and</span> <span class="nb">super</span><span class="p">(</span><span class="n">Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_should_compute_mask</span>

  <span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

    <span class="c1"># TODO(omalleyt): b/123540974 This function is not really safe to call</span>
    <span class="c1"># by itself because it will duplicate any updates and losses in graph</span>
    <span class="c1"># mode by `call`ing the Layers again.</span>
    <span class="n">output_tensors</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_internal_graph</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">t</span><span class="o">.</span><span class="n">_keras_mask</span><span class="p">,</span> <span class="n">output_tensors</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">layers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">trackable_layer_utils</span><span class="o">.</span><span class="n">filter_empty_layer_containers</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">get_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Retrieves a layer based on either its name (unique) or index.</span>

<span class="sd">    If `name` and `index` are both provided, `index` will take precedence.</span>
<span class="sd">    Indices are based on order of horizontal graph traversal (bottom-up).</span>

<span class="sd">    Arguments:</span>
<span class="sd">        name: String, name of layer.</span>
<span class="sd">        index: Integer, index of layer.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A layer instance.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: In case of invalid layer name or index.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO(fchollet): We could build a dictionary based on layer names</span>
    <span class="c1"># since they are constant, but we have not done that yet.</span>
    <span class="k">if</span> <span class="n">index</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">index</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Was asked to retrieve layer at index &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">index</span><span class="p">)</span> <span class="o">+</span>
                         <span class="s1">&#39; but model only has &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">))</span> <span class="o">+</span>
                         <span class="s1">&#39; layers.&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Provide either a layer name or layer index.&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">name</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">layer</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;No such layer: &#39;</span> <span class="o">+</span> <span class="n">name</span><span class="p">)</span>

  <span class="nd">@trackable</span><span class="o">.</span><span class="n">no_automatic_dependency_tracking</span>
  <span class="k">def</span> <span class="nf">_clear_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Used every step in eager to reset losses.&quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_eager_losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
      <span class="n">layer</span><span class="o">.</span><span class="n">_clear_losses</span><span class="p">()</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">trainable_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_assert_weights_created</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">trackable_layer_utils</span><span class="o">.</span><span class="n">gather_trainable_weights</span><span class="p">(</span>
        <span class="n">trainable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">sub_layers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">,</span>
        <span class="n">extra_variables</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">non_trainable_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_assert_weights_created</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">trackable_layer_utils</span><span class="o">.</span><span class="n">gather_non_trainable_weights</span><span class="p">(</span>
        <span class="n">trainable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainable</span><span class="p">,</span>
        <span class="n">sub_layers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">,</span>
        <span class="n">extra_variables</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_non_trainable_weights</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_weights</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_all_metrics_tensors</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the network&#39;s symbolic metric tensors.&quot;&quot;&quot;</span>
    <span class="c1"># TODO(psv): Remove this property.</span>
    <span class="n">metrics_tensors</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">Network</span><span class="p">):</span>
        <span class="n">metrics_tensors</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_all_metrics_tensors</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">metrics_tensors</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_metrics_tensors</span><span class="p">)</span>
    <span class="n">metrics_tensors</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_metrics_tensors</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">metrics_tensors</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">input_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Gets the network&#39;s input specs.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A list of `InputSpec` instances (one per input to the model)</span>
<span class="sd">            or a single instance if the model has only one input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># If subclassed model, can&#39;t assume anything.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span><span class="p">:</span>
      <span class="k">return</span> <span class="kc">None</span>

    <span class="n">specs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_layers</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">layer</span><span class="o">.</span><span class="n">input_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">specs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">input_spec</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span> <span class="o">+</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span>
                          <span class="s1">&#39; has an input_spec attribute that &#39;</span>
                          <span class="s1">&#39;is not a list. We expect a list. &#39;</span>
                          <span class="s1">&#39;Found input_spec = &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">input_spec</span><span class="p">))</span>
        <span class="n">specs</span> <span class="o">+=</span> <span class="n">layer</span><span class="o">.</span><span class="n">input_spec</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">specs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">specs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">specs</span>

  <span class="nd">@base_layer_utils</span><span class="o">.</span><span class="n">default</span>
  <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Builds the model based on input shapes received.</span>

<span class="sd">    This is to be used for subclassed models, which do not know at instantiation</span>
<span class="sd">    time what their inputs look like.</span>

<span class="sd">    This method only exists for users who want to call `model.build()` in a</span>
<span class="sd">    standalone way (as a substitute for calling the model on real data to</span>
<span class="sd">    build it). It will never be called by the framework (and thus it will</span>
<span class="sd">    never throw unexpected errors in an unrelated workflow).</span>

<span class="sd">    Args:</span>
<span class="sd">     input_shape: Single tuple, TensorShape, or list of shapes, where shapes</span>
<span class="sd">         are tuples, integers, or TensorShapes.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError:</span>
<span class="sd">        1. In case of invalid user-provided data (not of type tuple,</span>
<span class="sd">           list, or TensorShape).</span>
<span class="sd">        2. If the model requires call arguments that are agnostic</span>
<span class="sd">           to the input shapes (positional or kwarg in call signature).</span>
<span class="sd">        3. If not all layers were properly built.</span>
<span class="sd">        4. If float type inputs are not supported within the layers.</span>

<span class="sd">      In each of these cases, the user should build their model by calling it</span>
<span class="sd">      on real tensor data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>
      <span class="k">return</span>

    <span class="c1"># If subclass network</span>
    <span class="k">if</span> <span class="n">input_shape</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Input shape must be defined when calling build on a &#39;</span>
                       <span class="s1">&#39;model subclass network.&#39;</span><span class="p">)</span>
    <span class="n">valid_types</span> <span class="o">=</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">valid_types</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Specified input shape is not one of the valid types. &#39;</span>
                       <span class="s1">&#39;Please specify a batch input shape of type tuple or &#39;</span>
                       <span class="s1">&#39;list of input shapes. User provided &#39;</span>
                       <span class="s1">&#39;input type: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)))</span>

    <span class="k">if</span> <span class="n">input_shape</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
      <span class="c1"># We create placeholders for the `None`s in the shape and build the model</span>
      <span class="c1"># in a Graph. Since tf.Variable is compatible with both eager execution</span>
      <span class="c1"># and graph building, the variables created after building the model in</span>
      <span class="c1"># a Graph are still valid when executing eagerly.</span>
      <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">func_graph</span><span class="o">.</span><span class="n">FuncGraph</span><span class="p">(</span><span class="s1">&#39;build_graph&#39;</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">graph</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_graph</span><span class="p">()</span>
      <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
          <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">generate_placeholders_from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
               <span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">input_shape</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">x</span> <span class="o">=</span> <span class="n">base_layer_utils</span><span class="o">.</span><span class="n">generate_placeholders_from_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

        <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">call_signature</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>
        <span class="n">call_args</span> <span class="o">=</span> <span class="n">call_signature</span><span class="o">.</span><span class="n">args</span>
        <span class="c1"># Exclude `self`, `inputs`, and any argument with a default value.</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">call_args</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">call_signature</span><span class="o">.</span><span class="n">defaults</span><span class="p">:</span>
            <span class="n">call_args</span> <span class="o">=</span> <span class="n">call_args</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">call_signature</span><span class="o">.</span><span class="n">defaults</span><span class="p">)]</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">call_args</span> <span class="o">=</span> <span class="n">call_args</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
          <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">call_args</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">arg</span> <span class="o">==</span> <span class="s1">&#39;training&#39;</span><span class="p">:</span>
              <span class="c1"># Case where `training` is a positional arg with no default.</span>
              <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;training&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">else</span><span class="p">:</span>
              <span class="c1"># Has invalid call signature with unknown positional arguments.</span>
              <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                  <span class="s1">&#39;Currently, you cannot build your model if it has &#39;</span>
                  <span class="s1">&#39;positional or keyword arguments that are not &#39;</span>
                  <span class="s1">&#39;inputs to the model, but are required for its &#39;</span>
                  <span class="s1">&#39;`call` method. Instead, in order to instantiate &#39;</span>
                  <span class="s1">&#39;and build your model, `call` your model on real &#39;</span>
                  <span class="s1">&#39;tensor data with all expected call arguments.&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">call_args</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
          <span class="c1"># Signature without `inputs`.</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;You can only call `build` on a model if its `call` &#39;</span>
                           <span class="s1">&#39;method accepts an `inputs` argument.&#39;</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">InvalidArgumentError</span><span class="p">,</span> <span class="ne">TypeError</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;You cannot build your model by calling `build` &#39;</span>
                           <span class="s1">&#39;if your layers do not support float type inputs. &#39;</span>
                           <span class="s1">&#39;Instead, in order to instantiate and build your &#39;</span>
                           <span class="s1">&#39;model, `call` your model on real tensor data (of &#39;</span>
                           <span class="s1">&#39;the correct dtype).&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">:</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_track_layers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calls the model on new inputs.</span>

<span class="sd">    In this case `call` just reapplies</span>
<span class="sd">    all ops in the graph to the new inputs</span>
<span class="sd">    (e.g. build a new computational graph from the provided inputs).</span>

<span class="sd">    Arguments:</span>
<span class="sd">        inputs: A tensor or list of tensors.</span>
<span class="sd">        training: Boolean or boolean scalar tensor, indicating whether to run</span>
<span class="sd">          the `Network` in training mode or inference mode.</span>
<span class="sd">        mask: A mask or list of masks. A mask can be</span>
<span class="sd">            either a tensor or None (no mask).</span>

<span class="sd">    Returns:</span>
<span class="sd">        A tensor if there is a single output, or</span>
<span class="sd">        a list of tensors if there are more than one outputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s1">&#39;When subclassing the `Model` class, you should&#39;</span>
                                <span class="s1">&#39; implement a `call` method.&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_internal_graph</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span><span class="p">:</span>
      <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Network</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>

    <span class="c1"># Convert any shapes in tuple format to TensorShapes.</span>
    <span class="n">input_shape</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_shapes</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">to_tuples</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="p">))</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_layers</span><span class="p">)):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid input_shape argument &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">+</span>
                       <span class="s1">&#39;: model has &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_layers</span><span class="p">))</span> <span class="o">+</span>
                       <span class="s1">&#39; tensor inputs.&#39;</span><span class="p">)</span>

    <span class="n">cache_key</span> <span class="o">=</span> <span class="n">generic_utils</span><span class="o">.</span><span class="n">object_list_uid</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">cache_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape_cache</span><span class="p">:</span>
      <span class="c1"># Cache hit. Return shapes as TensorShapes.</span>
      <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape_cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span>

    <span class="n">layers_to_output_shapes</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">shape</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_layers</span><span class="p">,</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)):</span>
      <span class="c1"># It&#39;s an input layer: then `compute_output_shape` is identity,</span>
      <span class="c1"># and there is only one node and one tensor..</span>
      <span class="n">shape_key</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_0_0&#39;</span>
      <span class="n">layers_to_output_shapes</span><span class="p">[</span><span class="n">shape_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">shape</span>

    <span class="n">depth_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_depth</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">depth_keys</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Iterate over nodes, by depth level.</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">depth_keys</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depth_keys</span><span class="p">:</span>
        <span class="n">nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_depth</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
          <span class="c1"># This is always a single layer, never a list.</span>
          <span class="n">layer</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span>
          <span class="k">if</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_layers</span><span class="p">:</span>
            <span class="c1"># We&#39;ve already covered the input layers</span>
            <span class="c1"># a few lines above.</span>
            <span class="k">continue</span>
          <span class="c1"># Potentially redundant list,</span>
          <span class="c1"># same size as node.input_tensors.</span>
          <span class="n">layer_input_shapes</span> <span class="o">=</span> <span class="p">[]</span>
          <span class="k">for</span> <span class="n">inbound_layer</span><span class="p">,</span> <span class="n">node_id</span><span class="p">,</span> <span class="n">tensor_id</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">iterate_inbound</span><span class="p">():</span>
            <span class="n">input_layer_key</span> <span class="o">=</span> <span class="n">inbound_layer</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_</span><span class="si">%s</span><span class="s1">_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">node_id</span><span class="p">,</span>
                                                               <span class="n">tensor_id</span><span class="p">)</span>
            <span class="n">layer_input_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers_to_output_shapes</span><span class="p">[</span><span class="n">input_layer_key</span><span class="p">])</span>
          <span class="n">layer_input_shapes</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">inbound_layers</span><span class="p">,</span>
                                                     <span class="n">layer_input_shapes</span><span class="p">)</span>
          <span class="c1"># Layers expect shapes to be tuples for `compute_output_shape`.</span>
          <span class="n">layer_input_shapes</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_shapes</span><span class="p">(</span>
              <span class="n">layer_input_shapes</span><span class="p">,</span> <span class="n">to_tuples</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
          <span class="n">layer_output_shapes</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="n">layer_input_shapes</span><span class="p">)</span>
          <span class="c1"># Convert back to TensorShapes.</span>
          <span class="n">layer_output_shapes</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_shapes</span><span class="p">(</span>
              <span class="n">layer_output_shapes</span><span class="p">,</span> <span class="n">to_tuples</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

          <span class="n">node_index</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>  <span class="c1"># pylint: disable=protected-access</span>
          <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">shape</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">layer_output_shapes</span><span class="p">)):</span>
            <span class="n">shape_key</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_</span><span class="si">%s</span><span class="s1">_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
            <span class="n">layers_to_output_shapes</span><span class="p">[</span><span class="n">shape_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">shape</span>

      <span class="c1"># Read final output shapes from layers_to_output_shapes.</span>
      <span class="n">output_shapes</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_layers</span><span class="p">)):</span>
        <span class="n">layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_coordinates</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">shape_key</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;_</span><span class="si">%s</span><span class="s1">_</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span><span class="p">)</span>
        <span class="n">output_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layers_to_output_shapes</span><span class="p">[</span><span class="n">shape_key</span><span class="p">])</span>
      <span class="n">output_shapes</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nested_outputs</span><span class="p">,</span> <span class="n">output_shapes</span><span class="p">)</span>
      <span class="c1"># Store in cache.</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape_cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_shapes</span>

    <span class="c1"># Return shapes as TensorShapes.</span>
    <span class="k">return</span> <span class="n">output_shapes</span>

  <span class="k">def</span> <span class="nf">_run_internal_graph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Computes output tensors for new inputs.</span>

<span class="sd">    # Note:</span>
<span class="sd">        - Expects `inputs` to be a list (potentially with 1 element).</span>
<span class="sd">        - Can be run on non-Keras tensors.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        inputs: Tensor or nested structure of Tensors.</span>
<span class="sd">        training: Boolean learning phase.</span>
<span class="sd">        mask: (Optional) Tensor or nested structure of Tensors.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Two lists: output_tensors, output_masks</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Note: masking support is relevant mainly for Keras.</span>
    <span class="c1"># It cannot be factored out without having the fully reimplement the network</span>
    <span class="c1"># calling logic on the Keras side. We choose to incorporate it in</span>
    <span class="c1"># Network because 1) it may be useful to fully support in tf.layers in</span>
    <span class="c1"># the future and 2) Keras is a major user of Network.  If you don&#39;t</span>
    <span class="c1"># use masking, it does not interfere with regular behavior at all and you</span>
    <span class="c1"># can ignore it.</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">masks</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">))]</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">masks</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">input_t</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">masks</span><span class="p">):</span>
      <span class="n">input_t</span><span class="o">.</span><span class="n">_keras_mask</span> <span class="o">=</span> <span class="n">mask</span>

    <span class="c1"># Dictionary mapping reference tensors to computed tensors.</span>
    <span class="n">tensor_dict</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">masks</span><span class="p">):</span>
      <span class="n">tensor_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">))]</span> <span class="o">=</span> <span class="n">y</span>

    <span class="n">depth_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_depth</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">depth_keys</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1"># Ignore the InputLayers when computing the graph.</span>
    <span class="n">depth_keys</span> <span class="o">=</span> <span class="n">depth_keys</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depth_keys</span><span class="p">:</span>
      <span class="n">nodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_depth</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span>
      <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">:</span>
        <span class="c1"># This is always a single layer, never a list.</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span>

        <span class="k">if</span> <span class="nb">all</span><span class="p">(</span>
            <span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">tensor</span><span class="p">))</span> <span class="ow">in</span> <span class="n">tensor_dict</span>
            <span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">input_tensors</span><span class="p">)):</span>

          <span class="c1"># Call layer (reapplying ops to new inputs).</span>
          <span class="n">computed_tensors</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
              <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">tensor_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">t</span><span class="p">))],</span> <span class="n">node</span><span class="o">.</span><span class="n">input_tensors</span><span class="p">)</span>

          <span class="c1"># Ensure `training` and `mask` arg propagation if applicable.</span>
          <span class="n">kwargs</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">arguments</span> <span class="ow">or</span> <span class="p">{}</span>
          <span class="n">argspec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_call_argspecs</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">args</span>
          <span class="k">if</span> <span class="s1">&#39;training&#39;</span> <span class="ow">in</span> <span class="n">argspec</span><span class="p">:</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">&#39;training&#39;</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
          <span class="k">if</span> <span class="s1">&#39;mask&#39;</span> <span class="ow">in</span> <span class="n">argspec</span><span class="p">:</span>
            <span class="n">computed_masks</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
                <span class="n">computed_tensors</span><span class="p">)</span>
            <span class="n">kwargs</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">&#39;mask&#39;</span><span class="p">,</span> <span class="n">computed_masks</span><span class="p">)</span>

          <span class="c1"># Compute outputs.</span>
          <span class="n">output_tensors</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">computed_tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

          <span class="c1"># Update tensor_dict.</span>
          <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
              <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">output_tensors</span><span class="p">),</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">output_tensors</span><span class="p">)):</span>
            <span class="n">tensor_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">))]</span> <span class="o">=</span> <span class="n">y</span>

    <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">output_shapes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
      <span class="k">assert</span> <span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">))</span> <span class="ow">in</span> <span class="n">tensor_dict</span><span class="p">,</span> <span class="s1">&#39;Could not compute output &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
      <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor_dict</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">))]</span>
      <span class="n">output_shapes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
      <span class="n">output_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">output_shapes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
      <span class="n">input_shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">]</span>
      <span class="n">cache_key</span> <span class="o">=</span> <span class="n">generic_utils</span><span class="o">.</span><span class="n">object_list_uid</span><span class="p">(</span><span class="n">input_shapes</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_output_shape_cache</span><span class="p">[</span><span class="n">cache_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span>
          <span class="bp">self</span><span class="o">.</span><span class="n">_nested_outputs</span><span class="p">,</span> <span class="n">output_shapes</span><span class="p">)</span>

    <span class="n">output_tensors</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nested_outputs</span><span class="p">,</span> <span class="n">output_tensors</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output_tensors</span>

  <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="n">config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
    <span class="p">}</span>
    <span class="n">node_conversion_map</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
      <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="vm">__class__</span><span class="p">,</span> <span class="n">Network</span><span class="p">):</span>
        <span class="c1"># Networks start with a pre-existing node</span>
        <span class="c1"># linking their input to output.</span>
        <span class="n">kept_nodes</span> <span class="o">=</span> <span class="mi">1</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">kept_nodes</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="n">original_node_index</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">):</span>
        <span class="n">node_key</span> <span class="o">=</span> <span class="n">_make_node_key</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">original_node_index</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">node_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network_nodes</span><span class="p">:</span>
          <span class="n">node_conversion_map</span><span class="p">[</span><span class="n">node_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">kept_nodes</span>
          <span class="n">kept_nodes</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">layer_configs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>  <span class="c1"># From the earliest layers on.</span>
      <span class="n">layer_class_name</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
      <span class="n">layer_config</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
      <span class="n">filtered_inbound_nodes</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">original_node_index</span><span class="p">,</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">):</span>
        <span class="n">node_key</span> <span class="o">=</span> <span class="n">_make_node_key</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">original_node_index</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">node_key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network_nodes</span><span class="p">:</span>
          <span class="c1"># The node is relevant to the model:</span>
          <span class="c1"># add to filtered_inbound_nodes.</span>
          <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">arguments</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
              <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">arguments</span><span class="p">)</span>
              <span class="n">kwargs</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">arguments</span>
            <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
              <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                  <span class="s1">&#39;Layer &#39;</span> <span class="o">+</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span>
                  <span class="s1">&#39; was passed non-serializable keyword arguments: &#39;</span> <span class="o">+</span>
                  <span class="nb">str</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">arguments</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;. They will not be included &#39;</span>
                  <span class="s1">&#39;in the serialized model (and thus will be missing &#39;</span>
                  <span class="s1">&#39;at deserialization time).&#39;</span><span class="p">)</span>
              <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
          <span class="k">else</span><span class="p">:</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
          <span class="k">if</span> <span class="n">node</span><span class="o">.</span><span class="n">inbound_layers</span><span class="p">:</span>
            <span class="n">node_data</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">inbound_layer</span><span class="p">,</span> <span class="n">node_id</span><span class="p">,</span> <span class="n">tensor_id</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">iterate_inbound</span><span class="p">():</span>
              <span class="n">node_key</span> <span class="o">=</span> <span class="n">_make_node_key</span><span class="p">(</span><span class="n">inbound_layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">node_id</span><span class="p">)</span>
              <span class="n">new_node_index</span> <span class="o">=</span> <span class="n">node_conversion_map</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">node_key</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
              <span class="n">node_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                  <span class="n">tf_utils</span><span class="o">.</span><span class="n">ListWrapper</span><span class="p">(</span>
                      <span class="p">[</span><span class="n">inbound_layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">new_node_index</span><span class="p">,</span> <span class="n">tensor_id</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">]))</span>
            <span class="n">node_data</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">node_data</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">node_data</span><span class="p">):</span>
              <span class="n">node_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_data</span><span class="p">]</span>
            <span class="c1"># Convert ListWrapper to list for backwards compatible configs.</span>
            <span class="n">node_data</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_inner_node_data</span><span class="p">(</span><span class="n">node_data</span><span class="p">)</span>
            <span class="n">filtered_inbound_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node_data</span><span class="p">)</span>
      <span class="n">layer_configs</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
          <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
          <span class="s1">&#39;class_name&#39;</span><span class="p">:</span> <span class="n">layer_class_name</span><span class="p">,</span>
          <span class="s1">&#39;config&#39;</span><span class="p">:</span> <span class="n">layer_config</span><span class="p">,</span>
          <span class="s1">&#39;inbound_nodes&#39;</span><span class="p">:</span> <span class="n">filtered_inbound_nodes</span><span class="p">,</span>
      <span class="p">})</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;layers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">layer_configs</span>

    <span class="c1"># Gather info about inputs and outputs.</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_input_layers</span><span class="p">)):</span>
      <span class="n">layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_coordinates</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
      <span class="n">node_key</span> <span class="o">=</span> <span class="n">_make_node_key</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">node_index</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">node_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network_nodes</span><span class="p">:</span>
        <span class="k">continue</span>
      <span class="n">new_node_index</span> <span class="o">=</span> <span class="n">node_conversion_map</span><span class="p">[</span><span class="n">node_key</span><span class="p">]</span>
      <span class="n">model_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="n">tf_utils</span><span class="o">.</span><span class="n">ListWrapper</span><span class="p">([</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">new_node_index</span><span class="p">,</span> <span class="n">tensor_index</span><span class="p">]))</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nested_inputs</span><span class="p">,</span> <span class="n">model_inputs</span><span class="p">)</span>
    <span class="c1"># Preserve external Keras compat for Models with single input.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">):</span>
      <span class="n">model_inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_inputs</span><span class="p">]</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_inner_node_data</span><span class="p">(</span><span class="n">model_inputs</span><span class="p">)</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;input_layers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_inputs</span>

    <span class="n">model_outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_layers</span><span class="p">)):</span>
      <span class="n">layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_coordinates</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
      <span class="n">node_key</span> <span class="o">=</span> <span class="n">_make_node_key</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">node_index</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">node_key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_network_nodes</span><span class="p">:</span>
        <span class="k">continue</span>
      <span class="n">new_node_index</span> <span class="o">=</span> <span class="n">node_conversion_map</span><span class="p">[</span><span class="n">node_key</span><span class="p">]</span>
      <span class="n">model_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
          <span class="n">tf_utils</span><span class="o">.</span><span class="n">ListWrapper</span><span class="p">([</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">new_node_index</span><span class="p">,</span> <span class="n">tensor_index</span><span class="p">]))</span>
    <span class="n">model_outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_nested_outputs</span><span class="p">,</span> <span class="n">model_outputs</span><span class="p">)</span>
    <span class="c1"># Preserve external Keras compat for Models with single output.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">):</span>
      <span class="n">model_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_outputs</span><span class="p">]</span>
    <span class="n">model_outputs</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_inner_node_data</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">)</span>
    <span class="n">config</span><span class="p">[</span><span class="s1">&#39;output_layers&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_outputs</span>
    <span class="k">return</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

  <span class="nd">@classmethod</span>
  <span class="k">def</span> <span class="nf">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">custom_objects</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiates a Model from its config (output of `get_config()`).</span>

<span class="sd">    Arguments:</span>
<span class="sd">        config: Model config dictionary.</span>
<span class="sd">        custom_objects: Optional dictionary mapping names</span>
<span class="sd">            (strings) to custom classes or functions to be</span>
<span class="sd">            considered during deserialization.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A model instance.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: In case of improperly formatted config dict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Layer instances created during</span>
    <span class="c1"># the graph reconstruction process</span>
    <span class="n">created_layers</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Dictionary mapping layer instances to</span>
    <span class="c1"># node data that specifies a layer call.</span>
    <span class="c1"># It acts as a queue that maintains any unprocessed</span>
    <span class="c1"># layer call until it becomes possible to process it</span>
    <span class="c1"># (i.e. until the input tensors to the call all exist).</span>
    <span class="n">unprocessed_nodes</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">add_unprocessed_node</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">node_data</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">layer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">unprocessed_nodes</span><span class="p">:</span>
        <span class="n">unprocessed_nodes</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">node_data</span><span class="p">]</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">unprocessed_nodes</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">process_node</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">node_data</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Deserialize a node.</span>

<span class="sd">      Arguments:</span>
<span class="sd">          layer: layer instance.</span>
<span class="sd">          node_data: Nested structure of `ListWrapper`.</span>

<span class="sd">      Raises:</span>
<span class="sd">          ValueError: In case of improperly formatted `node_data`.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">input_tensors</span> <span class="o">=</span> <span class="p">[]</span>
      <span class="k">for</span> <span class="n">input_data</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">node_data</span><span class="p">):</span>
        <span class="n">input_data</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
        <span class="n">inbound_layer_name</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">inbound_node_index</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">inbound_tensor_index</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
          <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
          <span class="n">kwargs</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Improperly formatted model config.&#39;</span><span class="p">)</span>

        <span class="n">inbound_layer</span> <span class="o">=</span> <span class="n">created_layers</span><span class="p">[</span><span class="n">inbound_layer_name</span><span class="p">]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">inbound_layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">inbound_node_index</span><span class="p">:</span>
          <span class="n">add_unprocessed_node</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">node_data</span><span class="p">)</span>
          <span class="k">return</span>
        <span class="n">inbound_node</span> <span class="o">=</span> <span class="n">inbound_layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="n">inbound_node_index</span><span class="p">]</span>
        <span class="n">input_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">inbound_node</span><span class="o">.</span><span class="n">output_tensors</span><span class="p">)[</span><span class="n">inbound_tensor_index</span><span class="p">])</span>
      <span class="n">input_tensors</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">node_data</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">)</span>
      <span class="c1"># Call layer on its inputs, thus creating the node</span>
      <span class="c1"># and building the layer if needed.</span>
      <span class="k">if</span> <span class="n">input_tensors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Preserve compatibility with older configs.</span>
        <span class="n">flat_input_tensors</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">flat_input_tensors</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
          <span class="n">layer</span><span class="p">(</span><span class="n">flat_input_tensors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">layer</span><span class="p">(</span><span class="n">input_tensors</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">process_layer</span><span class="p">(</span><span class="n">layer_data</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Deserializes a layer, then call it on appropriate inputs.</span>

<span class="sd">      Arguments:</span>
<span class="sd">          layer_data: layer config dict.</span>

<span class="sd">      Raises:</span>
<span class="sd">          ValueError: In case of improperly formatted `layer_data` dict.</span>
<span class="sd">      &quot;&quot;&quot;</span>
      <span class="n">layer_name</span> <span class="o">=</span> <span class="n">layer_data</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span>

      <span class="c1"># Instantiate layer.</span>
      <span class="kn">from</span> <span class="nn">tensorflow.python.keras.layers</span> <span class="k">import</span> <span class="n">deserialize</span> <span class="k">as</span> <span class="n">deserialize_layer</span>  <span class="c1"># pylint: disable=g-import-not-at-top</span>

      <span class="n">layer</span> <span class="o">=</span> <span class="n">deserialize_layer</span><span class="p">(</span><span class="n">layer_data</span><span class="p">,</span> <span class="n">custom_objects</span><span class="o">=</span><span class="n">custom_objects</span><span class="p">)</span>
      <span class="n">created_layers</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">layer</span>

      <span class="c1"># Gather layer inputs and convert to `ListWrapper` objects.</span>
      <span class="n">inbound_nodes_data</span> <span class="o">=</span> <span class="n">layer_data</span><span class="p">[</span><span class="s1">&#39;inbound_nodes&#39;</span><span class="p">]</span>
      <span class="n">inbound_nodes_data</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_inner_node_data</span><span class="p">(</span>
          <span class="n">inbound_nodes_data</span><span class="p">,</span> <span class="n">wrap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
      <span class="k">for</span> <span class="n">node_data</span> <span class="ow">in</span> <span class="n">inbound_nodes_data</span><span class="p">:</span>
        <span class="c1"># We don&#39;t process nodes (i.e. make layer calls)</span>
        <span class="c1"># on the fly because the inbound node may not yet exist,</span>
        <span class="c1"># in case of layer shared at different topological depths</span>
        <span class="c1"># (e.g. a model such as A(B(A(B(x)))))</span>
        <span class="n">add_unprocessed_node</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">node_data</span><span class="p">)</span>

    <span class="c1"># First, we create all layers and enqueue nodes to be processed</span>
    <span class="k">for</span> <span class="n">layer_data</span> <span class="ow">in</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;layers&#39;</span><span class="p">]:</span>
      <span class="n">process_layer</span><span class="p">(</span><span class="n">layer_data</span><span class="p">)</span>
    <span class="c1"># Then we process nodes in order of layer depth.</span>
    <span class="c1"># Nodes that cannot yet be processed (if the inbound node</span>
    <span class="c1"># does not yet exist) are re-enqueued, and the process</span>
    <span class="c1"># is repeated until all nodes are processed.</span>
    <span class="k">while</span> <span class="n">unprocessed_nodes</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">layer_data</span> <span class="ow">in</span> <span class="n">config</span><span class="p">[</span><span class="s1">&#39;layers&#39;</span><span class="p">]:</span>
        <span class="n">layer</span> <span class="o">=</span> <span class="n">created_layers</span><span class="p">[</span><span class="n">layer_data</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]]</span>
        <span class="k">if</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">unprocessed_nodes</span><span class="p">:</span>
          <span class="k">for</span> <span class="n">node_data</span> <span class="ow">in</span> <span class="n">unprocessed_nodes</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">layer</span><span class="p">):</span>
            <span class="n">process_node</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">node_data</span><span class="p">)</span>

    <span class="n">name</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
    <span class="n">input_tensors</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">output_tensors</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">input_layers</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_inner_node_data</span><span class="p">(</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;input_layers&#39;</span><span class="p">],</span> <span class="n">wrap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">layer_data</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">input_layers</span><span class="p">):</span>
      <span class="n">layer_name</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span> <span class="o">=</span> <span class="n">layer_data</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
      <span class="k">assert</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="n">created_layers</span>
      <span class="n">layer</span> <span class="o">=</span> <span class="n">created_layers</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
      <span class="n">layer_output_tensors</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="n">node_index</span><span class="p">]</span><span class="o">.</span><span class="n">output_tensors</span>
      <span class="n">input_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">layer_output_tensors</span><span class="p">)[</span><span class="n">tensor_index</span><span class="p">])</span>

    <span class="n">output_layers</span> <span class="o">=</span> <span class="n">tf_utils</span><span class="o">.</span><span class="n">convert_inner_node_data</span><span class="p">(</span>
        <span class="n">config</span><span class="p">[</span><span class="s1">&#39;output_layers&#39;</span><span class="p">],</span> <span class="n">wrap</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">layer_data</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">output_layers</span><span class="p">):</span>
      <span class="n">layer_name</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span> <span class="o">=</span> <span class="n">layer_data</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span>
      <span class="k">assert</span> <span class="n">layer_name</span> <span class="ow">in</span> <span class="n">created_layers</span>
      <span class="n">layer</span> <span class="o">=</span> <span class="n">created_layers</span><span class="p">[</span><span class="n">layer_name</span><span class="p">]</span>
      <span class="n">layer_output_tensors</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="n">node_index</span><span class="p">]</span><span class="o">.</span><span class="n">output_tensors</span>
      <span class="n">output_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">layer_output_tensors</span><span class="p">)[</span><span class="n">tensor_index</span><span class="p">])</span>

    <span class="n">input_tensors</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">input_layers</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">)</span>
    <span class="n">output_tensors</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">pack_sequence_as</span><span class="p">(</span><span class="n">output_layers</span><span class="p">,</span> <span class="n">output_tensors</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_tensors</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_tensors</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

    <span class="c1"># Layers not connected to outputs, such as those added in `add_loss`.</span>
    <span class="n">ancillary_layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">layer</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">created_layers</span><span class="o">.</span><span class="n">values</span><span class="p">()</span> <span class="k">if</span> <span class="n">layer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="n">ancillary_layers</span><span class="p">:</span>
      <span class="n">model</span><span class="o">.</span><span class="n">_insert_layers</span><span class="p">(</span><span class="n">ancillary_layers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

  <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
           <span class="n">filepath</span><span class="p">,</span>
           <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">include_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">save_format</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Saves the model to Tensorflow SavedModel or a single HDF5 file.</span>

<span class="sd">    The savefile includes:</span>
<span class="sd">        - The model architecture, allowing to re-instantiate the model.</span>
<span class="sd">        - The model weights.</span>
<span class="sd">        - The state of the optimizer, allowing to resume training</span>
<span class="sd">            exactly where you left off.</span>

<span class="sd">    This allows you to save the entirety of the state of a model</span>
<span class="sd">    in a single file.</span>

<span class="sd">    Saved models can be reinstantiated via `keras.models.load_model`.</span>
<span class="sd">    The model returned by `load_model`</span>
<span class="sd">    is a compiled model ready to be used (unless the saved model</span>
<span class="sd">    was never compiled in the first place).</span>

<span class="sd">    Arguments:</span>
<span class="sd">        filepath: String, path to SavedModel or H5 file to save the model.</span>
<span class="sd">        overwrite: Whether to silently overwrite any existing file at the</span>
<span class="sd">            target location, or provide the user with a manual prompt.</span>
<span class="sd">        include_optimizer: If True, save optimizer&#39;s state together.</span>
<span class="sd">        save_format: Either &#39;tf&#39; or &#39;h5&#39;, indicating whether to save the model</span>
<span class="sd">          to Tensorflow SavedModel or HDF5. The default is currently &#39;h5&#39;, but</span>
<span class="sd">          will switch to &#39;tf&#39; in TensorFlow 2.0. The &#39;tf&#39; option is currently</span>
<span class="sd">          disabled (use `tf.keras.experimental.export_saved_model` instead).</span>

<span class="sd">    Example:</span>

<span class="sd">    ```python</span>
<span class="sd">    from keras.models import load_model</span>

<span class="sd">    model.save(&#39;my_model.h5&#39;)  # creates a HDF5 file &#39;my_model.h5&#39;</span>
<span class="sd">    del model  # deletes the existing model</span>

<span class="sd">    # returns a compiled model</span>
<span class="sd">    # identical to the previous one</span>
<span class="sd">    model = load_model(&#39;my_model.h5&#39;)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">saving</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">,</span> <span class="n">overwrite</span><span class="p">,</span> <span class="n">include_optimizer</span><span class="p">,</span> <span class="n">save_format</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">save_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">save_format</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Saves all layer weights.</span>

<span class="sd">    Either saves in HDF5 or in TensorFlow format based on the `save_format`</span>
<span class="sd">    argument.</span>

<span class="sd">    When saving in HDF5 format, the weight file has:</span>
<span class="sd">      - `layer_names` (attribute), a list of strings</span>
<span class="sd">          (ordered names of model layers).</span>
<span class="sd">      - For every layer, a `group` named `layer.name`</span>
<span class="sd">          - For every such layer group, a group attribute `weight_names`,</span>
<span class="sd">              a list of strings</span>
<span class="sd">              (ordered names of weights tensor of the layer).</span>
<span class="sd">          - For every weight in the layer, a dataset</span>
<span class="sd">              storing the weight value, named after the weight tensor.</span>

<span class="sd">    When saving in TensorFlow format, all objects referenced by the network are</span>
<span class="sd">    saved in the same format as `tf.train.Checkpoint`, including any `Layer`</span>
<span class="sd">    instances or `Optimizer` instances assigned to object attributes. For</span>
<span class="sd">    networks constructed from inputs and outputs using `tf.keras.Model(inputs,</span>
<span class="sd">    outputs)`, `Layer` instances used by the network are tracked/saved</span>
<span class="sd">    automatically. For user-defined classes which inherit from `tf.keras.Model`,</span>
<span class="sd">    `Layer` instances must be assigned to object attributes, typically in the</span>
<span class="sd">    constructor. See the documentation of `tf.train.Checkpoint` and</span>
<span class="sd">    `tf.keras.Model` for details.</span>

<span class="sd">    While the formats are the same, do not mix `save_weights` and</span>
<span class="sd">    `tf.train.Checkpoint`. Checkpoints saved by `Model.save_weights` should be</span>
<span class="sd">    loaded using `Model.load_weights`. Checkpoints saved using</span>
<span class="sd">    `tf.train.Checkpoint.save` should be restored using the corresponding</span>
<span class="sd">    `tf.train.Checkpoint.restore`. Prefer `tf.train.Checkpoint` over</span>
<span class="sd">    `save_weights` for training checkpoints.</span>

<span class="sd">    The TensorFlow format matches objects and variables by starting at a root</span>
<span class="sd">    object, `self` for `save_weights`, and greedily matching attribute</span>
<span class="sd">    names. For `Model.save` this is the `Model`, and for `Checkpoint.save` this</span>
<span class="sd">    is the `Checkpoint` even if the `Checkpoint` has a model attached. This</span>
<span class="sd">    means saving a `tf.keras.Model` using `save_weights` and loading into a</span>
<span class="sd">    `tf.train.Checkpoint` with a `Model` attached (or vice versa) will not match</span>
<span class="sd">    the `Model`&#39;s variables. See the [guide to training</span>
<span class="sd">    checkpoints](https://www.tensorflow.org/alpha/guide/checkpoints) for details</span>
<span class="sd">    on the TensorFlow format.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        filepath: String, path to the file to save the weights to. When saving</span>
<span class="sd">            in TensorFlow format, this is the prefix used for checkpoint files</span>
<span class="sd">            (multiple files are generated). Note that the &#39;.h5&#39; suffix causes</span>
<span class="sd">            weights to be saved in HDF5 format.</span>
<span class="sd">        overwrite: Whether to silently overwrite any existing file at the</span>
<span class="sd">            target location, or provide the user with a manual prompt.</span>
<span class="sd">        save_format: Either &#39;tf&#39; or &#39;h5&#39;. A `filepath` ending in &#39;.h5&#39; or</span>
<span class="sd">            &#39;.keras&#39; will default to HDF5 if `save_format` is `None`. Otherwise</span>
<span class="sd">            `None` defaults to &#39;tf&#39;.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ImportError: If h5py is not available when attempting to save in HDF5</span>
<span class="sd">            format.</span>
<span class="sd">        ValueError: For invalid/unknown format arguments.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_assert_weights_created</span><span class="p">()</span>
    <span class="n">filepath_is_h5</span> <span class="o">=</span> <span class="n">_is_hdf5_filepath</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">save_format</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">filepath_is_h5</span><span class="p">:</span>
        <span class="n">save_format</span> <span class="o">=</span> <span class="s1">&#39;h5&#39;</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">save_format</span> <span class="o">=</span> <span class="s1">&#39;tf&#39;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">user_format</span> <span class="o">=</span> <span class="n">save_format</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
      <span class="k">if</span> <span class="n">user_format</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;tensorflow&#39;</span><span class="p">,</span> <span class="s1">&#39;tf&#39;</span><span class="p">):</span>
        <span class="n">save_format</span> <span class="o">=</span> <span class="s1">&#39;tf&#39;</span>
      <span class="k">elif</span> <span class="n">user_format</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;hdf5&#39;</span><span class="p">,</span> <span class="s1">&#39;h5&#39;</span><span class="p">,</span> <span class="s1">&#39;keras&#39;</span><span class="p">):</span>
        <span class="n">save_format</span> <span class="o">=</span> <span class="s1">&#39;h5&#39;</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s1">&#39;Unknown format &quot;</span><span class="si">%s</span><span class="s1">&quot;. Was expecting one of {&quot;tf&quot;, &quot;h5&quot;}.&#39;</span> <span class="o">%</span> <span class="p">(</span>
                <span class="n">save_format</span><span class="p">,))</span>
    <span class="k">if</span> <span class="n">save_format</span> <span class="o">==</span> <span class="s1">&#39;tf&#39;</span> <span class="ow">and</span> <span class="n">filepath_is_h5</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
          <span class="p">(</span><span class="s1">&#39;save_weights got save_format=&quot;tf&quot;/&quot;tensorflow&quot;, but the &#39;</span>
           <span class="s1">&#39;filepath (&quot;</span><span class="si">%s</span><span class="s1">&quot;) looks like an HDF5 file. Omit the &quot;.h5&quot;/&quot;.keras&quot; &#39;</span>
           <span class="s1">&#39;when saving in TensorFlow format.&#39;</span><span class="p">)</span>
          <span class="o">%</span> <span class="n">filepath</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">save_format</span> <span class="o">==</span> <span class="s1">&#39;h5&#39;</span> <span class="ow">and</span> <span class="n">h5py</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
          <span class="s1">&#39;`save_weights` requires h5py when saving in hdf5.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">save_format</span> <span class="o">==</span> <span class="s1">&#39;tf&#39;</span><span class="p">:</span>
      <span class="n">check_filepath</span> <span class="o">=</span> <span class="n">filepath</span> <span class="o">+</span> <span class="s1">&#39;.index&#39;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">check_filepath</span> <span class="o">=</span> <span class="n">filepath</span>
    <span class="c1"># If file exists and should not be overwritten:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">overwrite</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">check_filepath</span><span class="p">):</span>
      <span class="n">proceed</span> <span class="o">=</span> <span class="n">ask_to_proceed_with_overwrite</span><span class="p">(</span><span class="n">check_filepath</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">proceed</span><span class="p">:</span>
        <span class="k">return</span>
    <span class="k">if</span> <span class="n">save_format</span> <span class="o">==</span> <span class="s1">&#39;h5&#39;</span><span class="p">:</span>
      <span class="k">with</span> <span class="n">h5py</span><span class="o">.</span><span class="n">File</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">saving</span><span class="o">.</span><span class="n">save_weights_to_hdf5_group</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="n">session</span> <span class="o">=</span> <span class="kc">None</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">session</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_session</span><span class="p">()</span>
      <span class="n">optimizer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;optimizer&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">optimizer</span>
          <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">trackable</span><span class="o">.</span><span class="n">Trackable</span><span class="p">)):</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="p">(</span><span class="s1">&#39;This model was compiled with a Keras optimizer (</span><span class="si">%s</span><span class="s1">) but is being &#39;</span>
             <span class="s1">&#39;saved in TensorFlow format with `save_weights`. The model</span><span class="se">\&#39;</span><span class="s1">s &#39;</span>
             <span class="s1">&#39;weights will be saved, but unlike with TensorFlow optimizers in &#39;</span>
             <span class="s1">&#39;the TensorFlow format the optimizer</span><span class="se">\&#39;</span><span class="s1">s state will not be &#39;</span>
             <span class="s1">&#39;saved.</span><span class="se">\n\n</span><span class="s1">Consider using a TensorFlow optimizer from `tf.train`.&#39;</span><span class="p">)</span>
            <span class="o">%</span> <span class="p">(</span><span class="n">optimizer</span><span class="p">,))</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_trackable_saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="n">session</span><span class="p">)</span>
      <span class="c1"># Record this checkpoint so it&#39;s visible from tf.train.latest_checkpoint.</span>
      <span class="n">checkpoint_management</span><span class="o">.</span><span class="n">update_checkpoint_state_internal</span><span class="p">(</span>
          <span class="n">save_dir</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">filepath</span><span class="p">),</span>
          <span class="n">model_checkpoint_path</span><span class="o">=</span><span class="n">filepath</span><span class="p">,</span>
          <span class="n">save_relative_paths</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
          <span class="n">all_model_checkpoint_paths</span><span class="o">=</span><span class="p">[</span><span class="n">filepath</span><span class="p">])</span>

  <span class="k">def</span> <span class="nf">load_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">,</span> <span class="n">by_name</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Loads all layer weights, either from a TensorFlow or an HDF5 weight file.</span>

<span class="sd">    If `by_name` is False weights are loaded based on the network&#39;s</span>
<span class="sd">    topology. This means the architecture should be the same as when the weights</span>
<span class="sd">    were saved.  Note that layers that don&#39;t have weights are not taken into</span>
<span class="sd">    account in the topological ordering, so adding or removing layers is fine as</span>
<span class="sd">    long as they don&#39;t have weights.</span>

<span class="sd">    If `by_name` is True, weights are loaded into layers only if they share the</span>
<span class="sd">    same name. This is useful for fine-tuning or transfer-learning models where</span>
<span class="sd">    some of the layers have changed.</span>

<span class="sd">    Only topological loading (`by_name=False`) is supported when loading weights</span>
<span class="sd">    from the TensorFlow format. Note that topological loading differs slightly</span>
<span class="sd">    between TensorFlow and HDF5 formats for user-defined classes inheriting from</span>
<span class="sd">    `tf.keras.Model`: HDF5 loads based on a flattened list of weights, while the</span>
<span class="sd">    TensorFlow format loads based on the object-local names of attributes to</span>
<span class="sd">    which layers are assigned in the `Model`&#39;s constructor.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        filepath: String, path to the weights file to load. For weight files in</span>
<span class="sd">            TensorFlow format, this is the file prefix (the same as was passed</span>
<span class="sd">            to `save_weights`).</span>
<span class="sd">        by_name: Boolean, whether to load weights by name or by topological</span>
<span class="sd">            order. Only topological loading is supported for weight files in</span>
<span class="sd">            TensorFlow format.</span>

<span class="sd">    Returns:</span>
<span class="sd">        When loading a weight file in TensorFlow format, returns the same status</span>
<span class="sd">        object as `tf.train.Checkpoint.restore`. When graph building, restore</span>
<span class="sd">        ops are run automatically as soon as the network is built (on first call</span>
<span class="sd">        for user-defined classes inheriting from `Model`, immediately if it is</span>
<span class="sd">        already built).</span>

<span class="sd">        When loading weights in HDF5 format, returns `None`.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ImportError: If h5py is not available and the weight file is in HDF5</span>
<span class="sd">            format.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">_is_hdf5_filepath</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
      <span class="n">save_format</span> <span class="o">=</span> <span class="s1">&#39;h5&#39;</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">pywrap_tensorflow</span><span class="o">.</span><span class="n">NewCheckpointReader</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
        <span class="n">save_format</span> <span class="o">=</span> <span class="s1">&#39;tf&#39;</span>
      <span class="k">except</span> <span class="n">errors_impl</span><span class="o">.</span><span class="n">DataLossError</span><span class="p">:</span>
        <span class="c1"># The checkpoint is not readable in TensorFlow format. Try HDF5.</span>
        <span class="n">save_format</span> <span class="o">=</span> <span class="s1">&#39;h5&#39;</span>
    <span class="k">if</span> <span class="n">save_format</span> <span class="o">==</span> <span class="s1">&#39;tf&#39;</span><span class="p">:</span>
      <span class="n">status</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trackable_saver</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">by_name</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="s1">&#39;Weights may only be loaded based on topology into Models when &#39;</span>
            <span class="s1">&#39;loading TensorFlow-formatted weights (got by_name=True to &#39;</span>
            <span class="s1">&#39;load_weights).&#39;</span><span class="p">)</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="n">context</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
        <span class="n">session</span> <span class="o">=</span> <span class="n">backend</span><span class="o">.</span><span class="n">get_session</span><span class="p">()</span>
        <span class="c1"># Restore existing variables (if any) immediately, and set up a</span>
        <span class="c1"># streaming restore for any variables created in the future.</span>
        <span class="n">trackable_utils</span><span class="o">.</span><span class="n">streaming_restore</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">status</span><span class="p">,</span> <span class="n">session</span><span class="o">=</span><span class="n">session</span><span class="p">)</span>
      <span class="n">status</span><span class="o">.</span><span class="n">assert_nontrivial_match</span><span class="p">()</span>
      <span class="k">return</span> <span class="n">status</span>
    <span class="k">if</span> <span class="n">h5py</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
          <span class="s1">&#39;`load_weights` requires h5py when loading weights from HDF5.&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
          <span class="s1">&#39;Unable to load weights saved in HDF5 format into a subclassed &#39;</span>
          <span class="s1">&#39;Model which has not created its variables yet. Call the Model &#39;</span>
          <span class="s1">&#39;first, then load the weights.&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_assert_weights_created</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">h5py</span><span class="o">.</span><span class="n">File</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
      <span class="k">if</span> <span class="s1">&#39;layer_names&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">f</span><span class="o">.</span><span class="n">attrs</span> <span class="ow">and</span> <span class="s1">&#39;model_weights&#39;</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">f</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="s1">&#39;model_weights&#39;</span><span class="p">]</span>
      <span class="k">if</span> <span class="n">by_name</span><span class="p">:</span>
        <span class="n">saving</span><span class="o">.</span><span class="n">load_weights_from_hdf5_group_by_name</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">saving</span><span class="o">.</span><span class="n">load_weights_from_hdf5_group</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_updated_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Util shared between different serialization methods.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Model config with Keras version information added.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="k">import</span> <span class="n">__version__</span> <span class="k">as</span> <span class="n">keras_version</span>  <span class="c1"># pylint: disable=g-import-not-at-top</span>

    <span class="n">config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
    <span class="n">model_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;class_name&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span>
        <span class="s1">&#39;config&#39;</span><span class="p">:</span> <span class="n">config</span><span class="p">,</span>
        <span class="s1">&#39;keras_version&#39;</span><span class="p">:</span> <span class="n">keras_version</span><span class="p">,</span>
        <span class="s1">&#39;backend&#39;</span><span class="p">:</span> <span class="n">backend</span><span class="o">.</span><span class="n">backend</span><span class="p">()</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">model_config</span>

  <span class="k">def</span> <span class="nf">to_json</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a JSON string containing the network configuration.</span>

<span class="sd">    To load a network from a JSON save file, use</span>
<span class="sd">    `keras.models.model_from_json(json_string, custom_objects={})`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        **kwargs: Additional keyword arguments</span>
<span class="sd">            to be passed to `json.dumps()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A JSON string.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">model_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_updated_config</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span>
        <span class="n">model_config</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">serialization</span><span class="o">.</span><span class="n">get_json_type</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">to_yaml</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns a yaml string containing the network configuration.</span>

<span class="sd">    To load a network from a yaml save file, use</span>
<span class="sd">    `keras.models.model_from_yaml(yaml_string, custom_objects={})`.</span>

<span class="sd">    `custom_objects` should be a dictionary mapping</span>
<span class="sd">    the names of custom losses / layers / etc to the corresponding</span>
<span class="sd">    functions / classes.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        **kwargs: Additional keyword arguments</span>
<span class="sd">            to be passed to `yaml.dump()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A YAML string.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ImportError: if yaml module is not found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">yaml</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span>
          <span class="s1">&#39;Requires yaml module installed (`pip install pyyaml`).&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_updated_config</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">line_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">positions</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">print_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Prints a string summary of the network.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        line_length: Total length of printed lines</span>
<span class="sd">            (e.g. set this to adapt the display to different</span>
<span class="sd">            terminal window sizes).</span>
<span class="sd">        positions: Relative or absolute positions of log elements</span>
<span class="sd">            in each line. If not provided,</span>
<span class="sd">            defaults to `[.33, .55, .67, 1.]`.</span>
<span class="sd">        print_fn: Print function to use. Defaults to `print`.</span>
<span class="sd">            It will be called on each line of the summary.</span>
<span class="sd">            You can set it to a custom function</span>
<span class="sd">            in order to capture the string summary.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if `summary()` is called before the model is built.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;This model has not yet been built. &#39;</span>
                       <span class="s1">&#39;Build the model first by calling `build()` or calling &#39;</span>
                       <span class="s1">&#39;`fit()` with some data, or specify &#39;</span>
                       <span class="s1">&#39;an `input_shape` argument in the first layer(s) for &#39;</span>
                       <span class="s1">&#39;automatic build.&#39;</span><span class="p">)</span>
    <span class="n">layer_utils</span><span class="o">.</span><span class="n">print_summary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                              <span class="n">line_length</span><span class="o">=</span><span class="n">line_length</span><span class="p">,</span>
                              <span class="n">positions</span><span class="o">=</span><span class="n">positions</span><span class="p">,</span>
                              <span class="n">print_fn</span><span class="o">=</span><span class="n">print_fn</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_validate_graph_inputs_and_outputs</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Validates the inputs and outputs of a Graph Network.&quot;&quot;&quot;</span>
    <span class="c1"># Check for redundancy in inputs.</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">))</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">):</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The list of inputs passed to the model &#39;</span>
                       <span class="s1">&#39;is redundant. &#39;</span>
                       <span class="s1">&#39;All inputs should only appear once.&#39;</span>
                       <span class="s1">&#39; Found: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
      <span class="c1"># Check that x has appropriate `_keras_history` metadata.</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_history&#39;</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Input tensors to a &#39;</span> <span class="o">+</span> <span class="n">cls_name</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span>
                         <span class="s1">&#39;must come from `tf.keras.Input`. &#39;</span>
                         <span class="s1">&#39;Received: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span>
                         <span class="s1">&#39; (missing previous layer metadata).&#39;</span><span class="p">)</span>
      <span class="c1"># Check that x is an input tensor.</span>
      <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">layer</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_keras_history</span><span class="o">.</span><span class="n">layer</span>
      <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="p">(</span>
          <span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span> <span class="ow">and</span> <span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">inbound_layers</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">cls_name</span> <span class="o">+</span> <span class="s1">&#39; inputs must come from &#39;</span>
                        <span class="s1">&#39;`tf.keras.Input` (thus holding past layer metadata), &#39;</span>
                        <span class="s1">&#39;they cannot be the output of &#39;</span>
                        <span class="s1">&#39;a previous non-Input layer. &#39;</span>
                        <span class="s1">&#39;Here, a tensor specified as &#39;</span>
                        <span class="s1">&#39;input to &quot;&#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;&quot; was not an Input tensor, &#39;</span>
                        <span class="s1">&#39;it was generated by layer &#39;</span> <span class="o">+</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;.</span><span class="se">\n</span><span class="s1">&#39;</span>
                        <span class="s1">&#39;Note that input tensors are &#39;</span>
                        <span class="s1">&#39;instantiated via `tensor = tf.keras.Input(shape)`.</span><span class="se">\n</span><span class="s1">&#39;</span>
                        <span class="s1">&#39;The tensor that caused the issue was: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>

    <span class="c1"># Check compatibility of batch sizes of Input Layers.</span>
    <span class="n">input_batch_sizes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">training_utils</span><span class="o">.</span><span class="n">get_static_batch_size</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">_keras_history</span><span class="o">.</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">inputs</span>
    <span class="p">]</span>
    <span class="n">consistent_batch_size</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">for</span> <span class="n">batch_size</span> <span class="ow">in</span> <span class="n">input_batch_sizes</span><span class="p">:</span>
      <span class="k">if</span> <span class="n">batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">consistent_batch_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span>
            <span class="n">batch_size</span> <span class="o">!=</span> <span class="n">consistent_batch_size</span><span class="p">):</span>
          <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The specified batch sizes of the Input Layers&#39;</span>
                           <span class="s1">&#39; are incompatible. Found batch sizes: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                               <span class="n">input_batch_sizes</span><span class="p">))</span>
        <span class="n">consistent_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>

    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
      <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;_keras_history&#39;</span><span class="p">):</span>
        <span class="n">cls_name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Output tensors to a &#39;</span> <span class="o">+</span> <span class="n">cls_name</span> <span class="o">+</span> <span class="s1">&#39; must be &#39;</span>
                         <span class="s1">&#39;the output of a TensorFlow `Layer` &#39;</span>
                         <span class="s1">&#39;(thus holding past layer metadata). Found: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

  <span class="k">def</span> <span class="nf">_insert_layers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">relevant_nodes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Inserts Layers into the Network after Network creation.</span>

<span class="sd">    This is only valid for Keras Graph Networks.  Layers added via this function</span>
<span class="sd">    will be included in the `call` computation and `get_config` of this Network.</span>
<span class="sd">    They will not be added to the Network&#39;s outputs.</span>


<span class="sd">    Arguments:</span>
<span class="sd">      layers: Arbitrary nested structure of Layers. Layers must be reachable</span>
<span class="sd">        from one or more of the `keras.Input` Tensors that correspond to this</span>
<span class="sd">        Network&#39;s inputs.</span>
<span class="sd">      relevant_nodes: Nodes from the Layers that should be considered part of</span>
<span class="sd">        this Network. If `None`, all Nodes will be considered part of this</span>
<span class="sd">        Network.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: If the layers depend on `Input`s not found in this Model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
    <span class="n">tf_utils</span><span class="o">.</span><span class="n">assert_no_legacy_layers</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
    <span class="n">node_to_depth</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">depth</span><span class="p">,</span> <span class="n">nodes</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_depth</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
      <span class="n">node_to_depth</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">node</span><span class="p">:</span> <span class="n">depth</span> <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span><span class="p">})</span>
    <span class="c1"># The nodes of these Layers that are relevant to this Network. If not</span>
    <span class="c1"># provided, assume all Nodes are relevant</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">relevant_nodes</span><span class="p">:</span>
      <span class="n">relevant_nodes</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">([</span><span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">])</span>
    <span class="n">network_nodes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">relevant_nodes</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">node_to_depth</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>

    <span class="k">def</span> <span class="nf">_get_min_depth</span><span class="p">(</span><span class="n">node</span><span class="p">):</span>
      <span class="sd">&quot;&quot;&quot;Gets the minimum depth at which node can be computed.&quot;&quot;&quot;</span>
      <span class="n">min_depth</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">node_id</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">iterate_inbound</span><span class="p">():</span>
        <span class="n">inbound_node</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="n">node_id</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">inbound_node</span> <span class="ow">in</span> <span class="n">node_to_depth</span><span class="p">:</span>
          <span class="n">min_depth</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">min_depth</span><span class="p">,</span> <span class="n">node_to_depth</span><span class="p">[</span><span class="n">inbound_node</span><span class="p">])</span>
        <span class="k">elif</span> <span class="n">inbound_node</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">network_nodes</span><span class="p">:</span>
          <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="c1"># Previous relevant nodes haven&#39;t been processed yet.</span>
          <span class="k">return</span> <span class="kc">None</span>
      <span class="c1"># New node is one shallower than its shallowest input.</span>
      <span class="k">return</span> <span class="n">min_depth</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="c1"># Insert nodes into `_nodes_by_depth` and other node attrs.</span>
    <span class="n">unprocessed_nodes</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">relevant_nodes</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">unprocessed_nodes</span><span class="p">:</span>
      <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
      <span class="c1"># Do a sanity check. This can occur if `Input`s from outside this Model</span>
      <span class="c1"># are being relied on.</span>
      <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">10000</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Layers could not be added due to missing &#39;</span>
                         <span class="s1">&#39;dependencies.&#39;</span><span class="p">)</span>

      <span class="n">node</span> <span class="o">=</span> <span class="n">unprocessed_nodes</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
      <span class="n">depth</span> <span class="o">=</span> <span class="n">_get_min_depth</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">depth</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">unprocessed_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">node_key</span> <span class="o">=</span> <span class="n">_make_node_key</span><span class="p">(</span>
            <span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">node</span><span class="p">))</span>
        <span class="n">node_to_depth</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">depth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_network_nodes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node_key</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_nodes_by_depth</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

    <span class="c1"># Insert layers into `_layer_by_depth` and other layer attrs.</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
      <span class="n">depth</span> <span class="o">=</span> <span class="nb">min</span><span class="p">([</span>
          <span class="n">node_to_depth</span><span class="p">[</span><span class="n">node</span><span class="p">]</span>
          <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">layer</span><span class="o">.</span><span class="n">inbound_nodes</span>
          <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">network_nodes</span>
      <span class="p">])</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_layers_by_depth</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
      <span class="bp">self</span><span class="o">.</span><span class="n">_layer_call_argspecs</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf_inspect</span><span class="o">.</span><span class="n">getfullargspec</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">call</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">_assert_weights_created</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Asserts that all the weights for the network have been created.</span>

<span class="sd">    For a non-dynamic network, the weights must already be created after the</span>
<span class="sd">    layer has been called. For a dynamic network, the exact list of weights can</span>
<span class="sd">    never be known for certain since it may change at any time during execution.</span>

<span class="sd">    We run this check right before accessing weights or getting the Numpy value</span>
<span class="sd">    for the current weights. Otherwise, if the layer has never been called,</span>
<span class="sd">    the user would just get an empty list, which is misleading.</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if the weights of the network has not yet been created.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dynamic</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_graph_network</span> <span class="ow">and</span>
        <span class="s1">&#39;build&#39;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__dict__</span> <span class="ow">and</span>
        <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">):</span>
      <span class="c1"># For any model that has customized build() method but hasn&#39;t</span>
      <span class="c1"># been invoked yet, this will cover both sequential and subclass model.</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Weights for model </span><span class="si">%s</span><span class="s1"> have not yet been created. &#39;</span>
                       <span class="s1">&#39;Weights are created when the Model is first called on &#39;</span>
                       <span class="s1">&#39;inputs or `build()` is called with an `input_shape`.&#39;</span> <span class="o">%</span>
                       <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="nd">@property</span>
  <span class="k">def</span> <span class="nf">_object_identifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;_tf_keras_network&#39;</span>


<span class="k">def</span> <span class="nf">_is_hdf5_filepath</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">filepath</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.h5&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">filepath</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.keras&#39;</span><span class="p">)</span> <span class="ow">or</span>
          <span class="n">filepath</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s1">&#39;.hdf5&#39;</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_make_node_key</span><span class="p">(</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">node_index</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">layer_name</span> <span class="o">+</span> <span class="s1">&#39;_ib-&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_map_graph_network</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;Validates a network&#39;s topology and gather its layers and nodes.</span>

<span class="sd">  Arguments:</span>
<span class="sd">    inputs: List of input tensors.</span>
<span class="sd">    outputs: List of outputs tensors.</span>

<span class="sd">  Returns:</span>
<span class="sd">    A tuple `(nodes, nodes_by_depth, layers, layers_by_depth)`.</span>
<span class="sd">    - nodes: list of Node instances.</span>
<span class="sd">    - nodes_by_depth: dict mapping ints (depth) to lists of node instances.</span>
<span class="sd">    - layers: list of Layer instances.</span>
<span class="sd">    - layers_by_depth: dict mapping ints (depth) to lists of layer instances.</span>

<span class="sd">  Raises:</span>
<span class="sd">    ValueError: In case the network is not valid (e.g. disconnected graph).</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="c1"># Network_nodes: set of nodes included in the graph of layers</span>
  <span class="c1"># (not all nodes included in the layers are relevant to the current graph).</span>
  <span class="n">network_nodes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>  <span class="c1"># ids of all nodes relevant to the Network</span>
  <span class="n">nodes_depths</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># dict {node: depth value}</span>
  <span class="n">layers_depths</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># dict {layer: depth value}</span>
  <span class="n">layer_indices</span> <span class="o">=</span> <span class="p">{}</span>  <span class="c1"># dict {layer: index in traversal}</span>
  <span class="n">nodes_in_decreasing_depth</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">def</span> <span class="nf">build_map</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span>
                <span class="n">finished_nodes</span><span class="p">,</span>
                <span class="n">nodes_in_progress</span><span class="p">,</span>
                <span class="n">layer</span><span class="p">,</span>
                <span class="n">node_index</span><span class="p">,</span>
                <span class="n">tensor_index</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Builds a map of the graph of layers.</span>

<span class="sd">    This recursively updates the map `layer_indices`,</span>
<span class="sd">    the list `nodes_in_decreasing_depth` and the set `network_nodes`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        tensor: Some tensor in a graph.</span>
<span class="sd">        finished_nodes: Set of nodes whose subgraphs have been traversed</span>
<span class="sd">            completely. Useful to prevent duplicated work.</span>
<span class="sd">        nodes_in_progress: Set of nodes that are currently active on the</span>
<span class="sd">            recursion stack. Useful to detect cycles.</span>
<span class="sd">        layer: Layer from which `tensor` comes from. If not provided,</span>
<span class="sd">            will be obtained from `tensor._keras_history`.</span>
<span class="sd">        node_index: Node index from which `tensor` comes from.</span>
<span class="sd">        tensor_index: Tensor_index from which `tensor` comes from.</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: if a cycle is detected.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">node</span> <span class="o">=</span> <span class="n">layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="n">node_index</span><span class="p">]</span>  <span class="c1"># pylint: disable=protected-access</span>

    <span class="c1"># Prevent cycles.</span>
    <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes_in_progress</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The tensor &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39; at layer &quot;&#39;</span> <span class="o">+</span>
                       <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;&quot; is part of a cycle.&#39;</span><span class="p">)</span>

    <span class="c1"># Don&#39;t repeat work for shared subgraphs</span>
    <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">finished_nodes</span><span class="p">:</span>
      <span class="k">return</span>

    <span class="n">node_key</span> <span class="o">=</span> <span class="n">_make_node_key</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">node_index</span><span class="p">)</span>
    <span class="c1"># Update network_nodes.</span>
    <span class="n">network_nodes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node_key</span><span class="p">)</span>

    <span class="c1"># Store the traversal order for layer sorting.</span>
    <span class="k">if</span> <span class="n">layer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">layer_indices</span><span class="p">:</span>
      <span class="n">layer_indices</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_indices</span><span class="p">)</span>

    <span class="n">nodes_in_progress</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

    <span class="c1"># Propagate to all previous tensors connected to this node.</span>
    <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">iterate_inbound</span><span class="p">():</span>
      <span class="n">build_map</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">finished_nodes</span><span class="p">,</span> <span class="n">nodes_in_progress</span><span class="p">,</span> <span class="n">layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span>
                <span class="n">tensor_index</span><span class="p">)</span>

    <span class="n">finished_nodes</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
    <span class="n">nodes_in_progress</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
    <span class="n">nodes_in_decreasing_depth</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

  <span class="n">finished_nodes</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="n">nodes_in_progress</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">:</span>
    <span class="n">layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">tensor_index</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">_keras_history</span>  <span class="c1"># pylint: disable=protected-access</span>
    <span class="n">build_map</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">finished_nodes</span><span class="p">,</span> <span class="n">nodes_in_progress</span><span class="p">,</span>
              <span class="n">layer</span><span class="o">=</span><span class="n">layer</span><span class="p">,</span>
              <span class="n">node_index</span><span class="o">=</span><span class="n">node_index</span><span class="p">,</span>
              <span class="n">tensor_index</span><span class="o">=</span><span class="n">tensor_index</span><span class="p">)</span>

  <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="n">nodes_in_decreasing_depth</span><span class="p">):</span>
    <span class="c1"># If the depth is not set, the node has no outbound nodes (depth 0).</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="n">nodes_depths</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Update the depth of the corresponding layer</span>
    <span class="n">previous_depth</span> <span class="o">=</span> <span class="n">layers_depths</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># If we&#39;ve seen this layer before at a higher depth,</span>
    <span class="c1"># we should use that depth instead of the node depth.</span>
    <span class="c1"># This is necessary for shared layers that have inputs at different</span>
    <span class="c1"># depth levels in the graph.</span>
    <span class="n">depth</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">previous_depth</span><span class="p">)</span>
    <span class="n">layers_depths</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">depth</span>
    <span class="n">nodes_depths</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="n">depth</span>

    <span class="c1"># Update the depth of inbound nodes.</span>
    <span class="c1"># The &quot;depth&quot; of a node is the max of the depths</span>
    <span class="c1"># of all layers it is connected to.</span>
    <span class="k">for</span> <span class="n">inbound_layer</span><span class="p">,</span> <span class="n">node_index</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">iterate_inbound</span><span class="p">():</span>
      <span class="n">inbound_node</span> <span class="o">=</span> <span class="n">inbound_layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="n">node_index</span><span class="p">]</span>  <span class="c1"># pylint: disable=protected-access</span>
      <span class="n">previous_depth</span> <span class="o">=</span> <span class="n">nodes_depths</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">inbound_node</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
      <span class="n">nodes_depths</span><span class="p">[</span><span class="n">inbound_node</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">previous_depth</span><span class="p">)</span>

  <span class="c1"># Handle inputs that are not connected to outputs.</span>
  <span class="k">for</span> <span class="n">input_t</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
    <span class="n">input_layer</span> <span class="o">=</span> <span class="n">input_t</span><span class="o">.</span><span class="n">_keras_history</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">input_layer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">layers_depths</span><span class="p">:</span>
      <span class="n">layers_depths</span><span class="p">[</span><span class="n">input_layer</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="n">layer_indices</span><span class="p">[</span><span class="n">input_layer</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
      <span class="n">nodes_depths</span><span class="p">[</span><span class="n">input_layer</span><span class="o">.</span><span class="n">_inbound_nodes</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="c1"># Build a dict {depth: list of nodes with this depth}</span>
  <span class="n">nodes_by_depth</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">nodes_depths</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">nodes_by_depth</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>

  <span class="c1"># Build a dict {depth: list of layers with this depth}</span>
  <span class="n">layers_by_depth</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">layer</span><span class="p">,</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">layers_depths</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">layers_by_depth</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>

  <span class="c1"># Get sorted list of layer depths.</span>
  <span class="n">depth_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">layers_by_depth</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
  <span class="n">depth_keys</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="c1"># Set self.layers and self._layers_by_depth.</span>
  <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depth_keys</span><span class="p">:</span>
    <span class="n">layers_for_depth</span> <span class="o">=</span> <span class="n">layers_by_depth</span><span class="p">[</span><span class="n">depth</span><span class="p">]</span>
    <span class="c1"># Network.layers needs to have a deterministic order:</span>
    <span class="c1"># here we order them by traversal order.</span>
    <span class="n">layers_for_depth</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">layer_indices</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">layers_for_depth</span><span class="p">)</span>

  <span class="c1"># Get sorted list of node depths.</span>
  <span class="n">depth_keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">nodes_by_depth</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
  <span class="n">depth_keys</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

  <span class="c1"># Check that all tensors required are computable.</span>
  <span class="c1"># computable_tensors: all tensors in the graph</span>
  <span class="c1"># that can be computed from the inputs provided.</span>
  <span class="n">computable_tensors</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
    <span class="n">computable_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

  <span class="n">layers_with_complete_input</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># To provide a better error msg.</span>
  <span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">depth_keys</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes_by_depth</span><span class="p">[</span><span class="n">depth</span><span class="p">]:</span>
      <span class="n">layer</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">outbound_layer</span>
      <span class="k">if</span> <span class="n">layer</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">input_tensors</span><span class="p">):</span>
          <span class="k">if</span> <span class="n">x</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">computable_tensors</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Graph disconnected: &#39;</span>
                             <span class="s1">&#39;cannot obtain value for tensor &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span>
                             <span class="s1">&#39; at layer &quot;&#39;</span> <span class="o">+</span> <span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;&quot;. &#39;</span>
                             <span class="s1">&#39;The following previous layers &#39;</span>
                             <span class="s1">&#39;were accessed without issue: &#39;</span> <span class="o">+</span>
                             <span class="nb">str</span><span class="p">(</span><span class="n">layers_with_complete_input</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">output_tensors</span><span class="p">):</span>
          <span class="n">computable_tensors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">layers_with_complete_input</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

  <span class="c1"># Ensure name unicity, which will be crucial for serialization</span>
  <span class="c1"># (since serialized nodes refer to layers by their name).</span>
  <span class="n">all_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">all_names</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">all_names</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">name</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
      <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;The name &quot;&#39;</span> <span class="o">+</span> <span class="n">name</span> <span class="o">+</span> <span class="s1">&#39;&quot; is used &#39;</span> <span class="o">+</span>
                       <span class="nb">str</span><span class="p">(</span><span class="n">all_names</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">name</span><span class="p">))</span> <span class="o">+</span> <span class="s1">&#39; times in the model. &#39;</span>
                       <span class="s1">&#39;All layer names should be unique.&#39;</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">network_nodes</span><span class="p">,</span> <span class="n">nodes_by_depth</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">layers_by_depth</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019 Zuru Tech HK Limited, All rights reserved.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>